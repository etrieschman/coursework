\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools,amssymb}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{layout}
\usepackage{geometry}
\usepackage{listings}
\usepackage{color}
\newcommand*{\vertbar}[1][10ex]{\rule[-1ex]{0.5pt}{#1}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\norm}[2]{\left\lVert#1\right\rVert_#2}
\newcommand{\abs}[1]{\lvert#1\rvert}

\title{CME302 class notes}
\author{Erich Trieschman}
\date{2021 Fall quarter}

\newcommand{\userMarginInMm}{8}
\geometry{
%  legal,
 left=\userMarginInMm mm,
 right=\userMarginInMm mm,
 top=\userMarginInMm mm,
 bottom=\userMarginInMm mm,
 footskip=5mm}

 \newcommand*\circled[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}

 \hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
    
\lstset{frame=tb, language=Python,
    aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3}

%HEADERS
% https://web.mit.edu/texsrc/source/latex/layouts/layman.pdf
% \textheight=650pt
% \textwidth=450pt
% \headheight=-25pt
% \oddsidemargin=5pt
% \footskip=25

\begin{document}
\maketitle
\tableofcontents

\section{Linear algebra review}

\subsection{Vector products}
The \textbf{inner product}, also known as the dot product, results in a scalar
\begin{itemize}
    \item $x^Ty = \sum x_i*y_i$
    \item $x^Ty = \norm{x}{2}\norm{y}{2}\cos\theta$
    \item $x^Ty = 0 \Leftrightarrow x \perp y$
\end{itemize}
The \textbf{outer product} results in a matrix. It is the outer sum of the two vectors, which can be of different lengths.

\subsection{Norms}
All norms, matrix or vector, satisfy
\begin{itemize}
    \item Only zero vector has zero norm: $\norm{x}{x} = 0 \Leftrightarrow x = 0$
    \item $\norm{\alpha x}{x} = \abs{\alpha}\norm{x}{x}$
    \item $\norm{x+y}{x} \leq \norm{x}{x} + \norm{y}{x}$ (Triangle inequality I), $\norm{x-y}{x} \geq \norm{x}{x} - \norm{y}{x}$ (Triangle inequality II)
\end{itemize}

\subsubsection{Vector norms}
Types of \textbf{vector norms}, $x \in \mathbb{R}^{n}$ (norm selection can give you solutions with different properties)
\begin{itemize}
    \item $\norm{x}{1} = \sum_{i=1}^n \abs{x_i}$
    \item $\norm{x}{2} = \sqrt{\sum_{i=1}^n (x_i)^2}$
    \item $\norm{x}{\infty} = \max_{i \in i,\dots, n} \abs{x_i}$
    \item $\norm{x}{p} = (\sum_{i=1}^n \abs{x_i}^p)^{\frac{1}{p}}$
\end{itemize}
\textbf{Cauchy-Schwarts Inequality:} $\abs{x^Ty}\leq \norm{x}{2}\norm{y}{2}$ (note equality when $x^Ty = 0$)\\ \\
\textbf{Holder's Inequality:} $\abs{x^Ty} \leq \norm{x}{p}\norm{y}{q}$, for $p, q$ , s.t. $\frac{1}{p} + \frac{1}{q} = 1$

\subsubsection{Matrix norms}
\noindent Types of \textbf{matrix norms}, $A \in \mathbb{R}^{n \times m}$
\begin{itemize}
    \item $\norm{A}{\infty} = \sup_{x\neq 0}\frac{\norm{Ax}{\infty}}{\norm{x}{\infty}} = \max_{\norm{x}{\infty}=1}\norm{Ax}{\infty} = \max_i\norm{a_i^T}{1}$
    \item $\norm{A}{p} = \sup_{x\neq 0}\frac{\norm{Ax}{p}}{\norm{x}{p}} = \max_{\norm{x}{p}=1}\norm{Ax}{p}$
    \item $\norm{A}{F} = \sqrt{\sum_{i,j} a_{ij}^2} = \sqrt{tr(AA^T)} = \sqrt{tr(A^TA)} = \sqrt{\sum_{k=1}^{min(m,n)}\sigma_k^2} $
\end{itemize}
\textbf{Submultiplicative inverse:} $\norm{AB}{p} \leq \norm{A}{p}\norm{B}{p}$. Note: this is not always true for Frobenius norms.\\ \\
\textbf{Induced p-norm: } $\norm{Ay}{p} \leq \norm{A}{p}\norm{y}{p}$\\ \\
\textbf{Orthogonally invariant:} Orthogonal matrices do not change the norms of vectors or matrices:
\begin{itemize}
    \item $\norm{Qx}{x} = \norm{x}{x}$
    \item $\norm{QA}{x} = \norm{A}{x}, x \in \{p, F\}$
\end{itemize}

\textbf{Other norm properties:}
\begin{itemize}
    \item $\norm{x}{\infty} \leq \norm{x}{2} \leq \sqrt{n}\norm{x}{\infty}$
    \item $\norm{A}{2} \leq \sqrt{m}\norm{A}{\infty}$
    \item $\norm{A}{\infty} \leq \sqrt{n}\norm{A}{2}$
\end{itemize}

\subsection{Matrix properties}
Matrices represent the following linear operations on a vector: Scaling, 1D reflection, 2D reflection (about a plane in N-dim space), Dimension reduction or increase ($A: x\in \mathbb{R}^m \rightarrow y = Ax \in \mathbb{R}^n$)


\subsubsection{Determinant}
The \textbf{determinant} represents how the volume of a hypercube is transformed by the matrix.
\begin{itemize}
    \item For square matrix, $det(\alpha A) = \alpha^ndet(A)$
    \item For square matrices, $det(AB) = det(A)det(B)$
    \item $det(A) = det(A^T)$
    \item $det(A^{-1}) = \frac{1}{det(A)}$
    \item For square matrix, $A$ singular $\Leftrightarrow det(A) = 0 \Leftrightarrow$ columns of $A$ are not linearly independent
\end{itemize}

\subsubsection{Trace}
The trace of a matrix $A \in \mathbb{R}^{mxn}, tr(A)$, is equal to the sum of the entries in its diagonal, $tr(A) = \sum_{i = 1}^n a_{ii}$. And a few properties of the trace:
\begin{itemize}
    \item $tr(A) = tr(A^T)$
    \item $tr(A + \alpha B) = tr(A) + \alpha tr(B)$
    \item Trace is invariant under cyclic permutations, that is $tr(ABCD) = tr(BCDA) = tr(CDAB) = tr(DABC)$
    \item For two vectors, $u, v \in \mathbb{R}, tr(uv^T) = v^Tu$
\end{itemize}

\subsubsection{Inverses and transposes}
The inverse of the transpose is the transpose of the inverse:
\begin{itemize}
    \item $A^T(A^{-1})^T = (A^{-1}A)^T = I^T = I$
    \item $(A^{-1})^TA^T = (AA^{-1})^T = I^T = I$
\end{itemize}
\subsubsection{Sherman-Morrison-Woodbury formula} 
for $A\in \mathbb{R}^{n\times n}, U,V \in \mathbb{R}^{n\times k}$
\begin{equation*}
    (A+UV^T)^{-1} = A^{-1} - A^{-1}U(I+V^TA^{-1}U)^{-1}V^TA^{-1} 
\end{equation*}
The significance of this formula is that you can compute the inverse of the sum of two matrices using the inverse of a known matrix, $A$, and the inverse of a much smaller matrix (assuming $k<n$) in $(I + V^TA^{-1}U)$\\ \\
\textbf{Proof:} begin with the inverse of the $LHS$ multiplied by the $RHS$: $(A+UV^T) (A^{-1} - A^{-1}U(I+V^TA^{-1}U)^{-1}V^TA^{-1})$. Next perform matrix multiplication. The end result will be $I$, implying that the $RHS$ is an inverse of $(A+UV^T)$


\subsection{Matrix multiplication}
\begin{align*}
\textrm{Show: } AB = a_1b_1^T + a_2b_2^T + ... + a_nb_n^T, A,B \in \mathbb{R} \\
   \textrm{Let }
   A = \begin{bmatrix}
        \vert & \vert& & \vert \\
        a_1 & a_2 & \dots & a_n   \\
        \vert & \vert & & \vert \end{bmatrix}, 
    B = \begin{bmatrix}
        \horzbar & b_1^T & \horzbar \\
        \horzbar & b_2^T & \horzbar \\
         & \vdots &  \\ 
        \horzbar & b_n^T & \horzbar
    \end{bmatrix}
\end{align*}
\begin{align*}
    a_1b_1^T &= \begin{bmatrix}
        a_{11}b_{11} & \dots & a_{11}b_{1n} \\
        \vdots &  & \vdots \\
        a_{n1}b_{11} & \dots & a_{n1}b_{1n}
    \end{bmatrix} \Rightarrow
    \sum_{i=1}^{n} a_ib_i^T = \begin{bmatrix}
        \sum a_{1i}b_{i1} & \dots & \sum a_{1i}b_{in} \\
        \vdots &   & \vdots \\
        \sum a_{ni}b_{i1} &  \dots & \sum a_{ni}b_{in}
    \end{bmatrix} \Rightarrow AB
\end{align*}

\subsection{Orthogonal matrices}
An orthogonal matrix, $Q$ is a matrix whose columns are orthonormal. That is, $q_i^Tq_j = 1$ for $i=j$, and $q_i^Tq_j = 0$ for $i\neq j$. Equivalently, $Q^TQ = I$. For square matrices, $Q^TQ = QQ^T = I$

\subsection{Projections, reflections, and rotations}
\subsubsection{Projections}
A projection, $v$, of vector $x$ onto vector $y$ can be written in the form
\begin{equation*}
    v = \frac{y^Tx}{y^Ty}y
\end{equation*}
Which can be interpreted as the portion of $x$ in the direction of $y$ ($y^Tx$), times the direction of $y$, divided by the length of $y$ twice ($y^Ty = \norm{y}{2}^2$), since $y$ appears in the dot product and in the vector. Observe, the denominator would be $1$ if $y$ were a unit vector\\

\noindent \textbf{Projection matrices} are square matrices, $P$, s.t., $P^2 = P$. 

\subsubsection{Reflection}
\begin{itemize}
    \item $P$ is a reflection matrix $\Leftrightarrow P^2 = I$
    \item $P$ can be written in the form $P = I - \beta vv^T$, with $\beta = \frac{2}{v^Tv}$, and $v$ the vector orthogonal to the line/plane of reflection
    \item It can be shown that $Px = x \Leftrightarrow v^Tx = 0$. These x are called the "fixed points" of $P$
\end{itemize}

% SYMMETRIC POSITIVE DEFINITE
\subsection{Symmetric Positive Definite (SPD) Matrices}
For \textbf{$A$, SPD,} i) $A = A^T$, ii) $x^TAx > 0$  $\forall x \neq 0$, iii) $a_{ii} > 0$, iv) $\lambda(A) \geq 0$, v) for $B$ nonsingular, $B^TAB$ is also SPD.\\ \\
When proving properties of SPDs, use the \textbf{following tricks:} i) Multiply by $e_i$ since $e_i \neq 0$, ii) Use matrix transpose property, $x^TA^T = (Ax)^T$ to rearrange formulas

\subsubsection{$B^TAB$ is also SPD} 
If $A$ SPD $\Rightarrow B^TAB$ SPD for $B$ nonsingular:
\begin{equation*}
    x^TB^TABx = (Bx)^TA(Bx) > 0, \textrm{(since $B$ nonsingular $\Rightarrow Bx \neq 0$)}
\end{equation*}

% EIGENVALUES
\subsection{Eigenvalues}
Observe by definition $Ax= \lambda x \longleftrightarrow Ax - \lambda x = 0 \longleftrightarrow (A - \lambda I)x = 0$.\\ \\ 
To find lambda, we solve for the system of equations to satisfy $(A - \lambda I)x = 0$\\ \\
The \textbf{algebraic multiplicity} of an eigenvalue, $\lambda_i$, is the number of times that $\lambda_1$ appears in $\lambda(A)$\\
The \textbf{geometric multiplicity} of an eigenvalue, $\lambda_i$, is the dimension of the space spanned by the eigenvectors of $\lambda_i$\\ \\
Other \textbf{eigenvalue properties}
\begin{itemize}
    \item $\lambda(A) = \lambda(A^T)$
    \item Courant-Fischer minmax theorem: $\lambda_1 = \max_{x \neq 0}\frac{x^TAx}{\norm{x}{2}^2}$
\end{itemize}

% DETERMINANTS/TRACE
\subsubsection{Determinants and trace}
\begin{align*}
    det(A) = \prod_{i=1}^n \lambda_i & & tr(A) = \sum_{i=1}^n \lambda_i
\end{align*}


% TRIANGULAR MATRICES
\subsubsection{Triangular matrices}
For $T$ triangular, the eigenvalues appear on the diagonal: $t_{ii} = \lambda_i, \forall i \in \{1,\dots, n\}$\\
\textbf{Corollary:} $T$ nonsingular $\Leftrightarrow$ all $t_{ii} \neq 0$

% GERSHGORIN DISC
\subsubsection{Gershgorin disc theorem}
Gershgorin disc, $\mathbb{D}_i$, defined
\begin{equation*}
    \mathbb{D}_i = \{z \in \mathbb{C} \mid \lvert z - a_{ii}\rvert \leq \sum_{j \neq i} \lvert a_{ij}\rvert\}
\end{equation*}
All eigenvalues of $A$, $\lambda(A) \in \mathbb{C}$ are located in one of its Gershgorin discs. \textbf{Proof:}

\begin{align*}
    Ax = \lambda x \longleftrightarrow (A - \lambda I)x = 0 &\longleftrightarrow \sum_{j \neq i} a_{ij}x_j + (a_{ii} - \lambda)x_i = 0, \; \forall \space i \in \{1, \dots, n\}\\
    \textrm{Choose } i \; s.t. \lvert x_i\rvert  = \max_{i} \lvert x_i \rvert  \\
    \lvert (a_{ii} - \lambda) \rvert &= \lvert \sum_{j \neq i} \frac{a_{ij}x_j}{x_i}\rvert
    \leq \sum_{j \neq i} \lvert \frac{a_{ij}x_j}{x_i}\rvert \; \textrm{, by triangle inequality}\\
    \lvert (\lambda - a_{ii}) \rvert &\leq \sum_{j \neq i} \lvert a_{ij}\rvert 
    \textrm{, since $\lvert \frac{x_j}{x_i} \rvert \leq 1$}
\end{align*}

% DECOMPOSITIONS
\section{Matrix Decompositions}
% SCHUR DECOMP
\subsection{Schur Decomposition}
For any $A \in \mathbb{C}^{n \times n}$, $A = QTQ^H$, where $Q$ unitary $(Q^HQ = I), Q \in \mathbb{C}^{n \times n}$, $T$ upper triangular\\ \\
When $A \in \mathbb{R}^{n \times n}$, $A = QTQ^T$, where $Q$ orthogonal $(Q^TQ = I), Q \in \mathbb{R}^{n \times n}$, $T$ upper triangular\\ \\
Note: If $T$ is relaxed from strict upper triangular to block upper triangular (blocks of $2\times 2$ or $1 \times 1$ on the diagonal), then $Q$ can be selected to be in $\mathbb{R}^{n\times n}$.


% EIGENVALUE DECOMP
\subsection{Eigenvalue Decomposition}
For $A$ diagonalizable ($A\in \mathbb{R}^{n\times n}$ with $n$ linearly independent eigenvectors), it can be decomposed as
\begin{equation*}
    A = X \Lambda X^{-1} \textrm{, where $\Lambda$ a diagonal matrix of the eigenvalues of $A$}
\end{equation*}
For $A$ real symmetric, $A$ can be decomposed as $A = Q\Lambda Q^T, Q$ orthogonal\\ \\
For $A$ unitarily diagonalizable ($\Leftrightarrow$ normal: $A^HA = AA^H$), $A= Q\Lambda Q^H, Q$ unitary. When $A$ complex Hermitian ($A = A^H$), $\Lambda \in \mathbb{R}$


% SINGULAR VALUE DECOMP
\subsection{Singular Value Decomposition}
\textbf{Definition:} For any $A \in \mathbb{C}^{m\times n}$ there exist two unitary matrices, $U \in \mathbb{C}^{m \times m}$ and $V \in \mathbb{C}^{n \times n}$, and a diagonal matrix $\Sigma \in \mathbb{R}^{m \times n}$ such that $A = U\Sigma V^H$. When $A \in \mathbb{R}^{m \times n}$, $A = U\Sigma V^T$ with $U, V, \Sigma \in \mathbb{R}$ \\ \\
The singular values, $\sigma_i$ of $\Sigma$ are always $\geq0$. And by convention, they're ordered in decreasing order, so $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_n$ \\ \\
\textbf{Motivation:} Consider the action of a matrix, $A$ on a sphere. $A$ maps the sphere to a hyperellipsoid, $E$
\begin{itemize}
    \item The lengths of the semi-axes of $E$ are denoted $\sigma_1, \dots, \sigma_n$ called \textbf{singular values} of $A$
    \item The directions of the semi axes are denoted by unit vectors, $u_1, \dots, u_n$ called \textbf{left singular vectors} of $A$
    \item For each $u_i$ there is some unit vector $v_i$ so that $Av_i = \sigma_iu_i$. The vectors $v_1, \dots, v_n$ are called the \textbf{right singular vectors}
\end{itemize}
\textbf{Derivation:} Observe $A^TA$ symmetric: $(A^TA)^T = A^TA$
\begin{align*}
    A^TA \textrm{ symmetric} &\Rightarrow \exists \; Q \textrm{ orthogonal and } \Lambda \textrm{ diagonal matrix of $\lambda_i$ s.t., }\\
    A^TA & = Q\Lambda Q^T\\
    Q^TA^TAQ & = Q^TQ\Lambda Q^TQ\\
    (AQ)^T(AQ) & = \Lambda \textrm{, note $AQ$ is orthogonal, but not scaled to 1. Instead, each row is} \\
    &\textrm{scaled to the eigenvalue in that row: }\lambda_i  = \norm{Aq_i}{2}^2\\
    \\
    \textrm{When $A$ is full rank,}&\\
    A &= AQQ^T\\
    &= (AQ) Q^T\\
    &= AQD^{-1}DQ^T \textrm{, where } D = \begin{bmatrix} \sqrt{\lambda_1} & \dots & 0\\ 
        \vdots & & \vdots\\ 0 & \dots & \sqrt{\lambda_n} \end{bmatrix} \textrm{ and } D^{-1} = \begin{bmatrix} \frac{1}{\sqrt{\lambda_1}} & \dots & 0\\ 
        \vdots & & \vdots\\ 0 & \dots & \frac{1}{\sqrt{\lambda_n}} \end{bmatrix}\\
    A &= U\Sigma V^T \textrm{, where } U  = AQD^{-1}, \Sigma = D, V^T = Q^T\\
    \\
    \textrm{When $A$ is not full rank,}&\textrm{ this does not hold since $\lambda_i = 0$ for some $i$ so we cannot construct $U$ with $D^{-1}$}\\
    \textrm{Start with } AQ &= \begin{bmatrix} \vert & & \vert & \vert & & \vert\\ 
        r_1 & \dots & r_r & 0 & \dots & 0\\
        \vert & & \vert & \vert & & \vert\end{bmatrix}\\
        A &= AQD^{-1}DQ^T \textrm{, where } D = \begin{bmatrix} \sqrt{\lambda_1} & \dots & 0 \\ 
        \vdots & & \vdots\\ 0 & \dots & \sqrt{\lambda_r} & & \\ 
        \vdots & & \vdots & I\\
        0 & \dots & 0 \end{bmatrix} \textrm{ (observe this matrix has inverse, $D^{-1}$)}\\
    A &= U\Sigma V^T \textrm{, where }\\
    &U  = \textrm{[left $r$ columns of $AQ] \; \times$ [ upper-left diagonal block of $D^{-1} \in \mathbb{R}^{r\times r}$], }\\
    &\Sigma = \textrm{[upper-left diagonal block of } D \in \mathbb{R}^{r\times r}]\\
    &V^T = [\textrm{left block of $Q$, or upper block of $Q^T$}]
\end{align*}
And a few properties and remarks of $A \in \mathbb{R}^{n\times m}$ SVD
\begin{itemize}
    \item $\norm{A}{2} = \sigma_1$; $\norm{A^{-1}}{2} = \frac{1}{\sigma_n}$ when $A$ nonsingular; $\norm{A}{F} = \sqrt{\sum_i^{min\{n,m\}}\sigma_i^2}$
    \item When $A$ symmetric, $\sigma_i = \abs{\lambda_i}$; When $A$ orthogonal, $\sigma_1 = \dots = \sigma_n = 1$
    \item The eigenvalues of $A^TA$ and $AA^T$ are the squares of the singular values of A, $\sigma_1^2, \dots, \sigma_n^2$
    \item By construction, $V$ contains the eigenvectors of $A^TA$ and $U$ contains the eigenvectors of $AA^T$, so $A^TAv_i = \sigma_i^2v_i$ and $AA^Tu_i = \sigma_i^2u_i$
    \item \textbf{Condition number}, $\kappa(A) = \norm{A}{2}\norm{A^{-1}}{2} = \frac{\sigma_1}{\sigma_n}$
\end{itemize}

% ERROR ANALYSIS
\section{Error analysis}
\subsection{Floating point arithmetic}
The cause of most roundoff errors steps from addition/subtraction resulting in lower floating point precision. General floating point number equation:
\begin{equation*}
    \pm (\sum_{i=1}^{t-1} d_i\beta^{-i})\beta^e
\end{equation*}
Where 
\begin{itemize}
    \item$\beta$ is the base (in floating point computation, $\beta=2$)
    \item $d_0\geq1$, and $d_i\leq \beta - 1$. 
    \item $e$ is called the \textbf{exponent}, this is the location of the decimal place. 
    \item $t-1$ in the summand is called the \textbf{precision} and indicates the number of digits (in base $\beta$) that can be stored with the number. 
    \item Lastly, the part of the equation in the parenthesis is referred to as the \textbf{significand} or \textbf{mantissa}
\end{itemize}


% UNIT ROUNDOFF
\subsection{Unit roundoff}
The \textbf{unit roundoff} for a floating-point number is 
\begin{equation*}
    u = \frac{1}{2} \times \beta^{-(t-1)} \textrm{ (distance between the smallest digits stored in a floating-point number)}
\end{equation*}
For double precision floating point numbers (64 bits), $u \approx 10^{-16}$\\
The \textbf{floating point truncation operator}, $fl(a)$, takes as input $a$ and returns the nearest floating point, $fl(a)$. Observe
\begin{equation*}
    fl(a+b) = a+b + \epsilon(a+b), \; \abs{\epsilon} \leq u \textrm{, the unit roundoff}
\end{equation*}
To \textbf{prove} this inequality, i) write $fl(x)$ and $x$ using floating point equations, ii) show the difference between these numbers is bounded by the smallest bit represented by $fl(x)$, iii) The $\frac{1}{2}$ enters the equation as a bound on the selection of the last digit of $fl(x)$ to approximate $x$.

% ERROR ANALYSIS
\subsection{Forward/Backward error analysis}
\textbf{Forward error analysis} looks to create bounds between the computed quantity $\Tilde{f}(A, b)$ and true value $f(A,b)$. The forward error is $\norm{\Tilde{f}(x) - f(x)}{p}$. i.e., What is the error in the solution computed with our algorithm? This is difficult to compute.\\ \\
\textbf{Backward error analysis} tries to find the error in $A$ that leads to observed answer $\tilde{x}$, $\Tilde{E}$ such that $(A + \Tilde{E})\tilde{x} = b$. i.e., what is the problem that our algorithm actually solved? An algorithm is regarded as \textit{backward stable} if $\norm{E}{p} \in O(u)$ \\ \\ 
The relative sensitivity of a problem is often called the \textbf{conditioning} of the problem
\begin{itemize}
    \item Sensitivity: $\frac{\norm{\Tilde{f}(x) - f(x)}{p}}{\norm{\Tilde{x} - x}{p}}$
    \item Relative sensitivity: $\frac{\norm{\Tilde{f}(x) - f(x)}{p}\norm{x}{p}}{\norm{\Tilde{x} - x}{p}\norm{f(x)}{p}}$
\end{itemize}


% LU FACTORIZATIONS
\section{LU Factorization}
The LU factorization makes it computationally easier to solve linear equations If we can decompose a matrix, $A$, into a product of a lower triangular matrix, $L$, and an upper triangular matrix, $U$, then to solve $Ax=b$, we can start by solving $Lz=b$, and then $Ux=z$. $x$, here, is the solution!

% BASIC ALGORITHM FOR LU FACTORIZATION
\subsection{Basic algorithm}
We construct matrices $L$ and $U$ by iteratively subtracting outer products of vectors that sequentially "zero-out" the rows and columns of $A$. We know $LU = l_1u_1^T + \dots + l_nu_n^T$, and when $l_1, u_1^T$ are from lower/upper respectively, $LU - l_1u_1^T$ yields a matrix with zeros in the first row and column. We use this principle for the basic algorithm 
\begin{itemize}
    \item Construct $u_1^T$ equal to the first row of $A, a_1^T$
    \item Construct $l_1$ equal to each of the elements in the first column of $A, a_1$, divided by $a_{11}$, the "pivot"
    \item Calculate $A' \leftarrow A - l_1u_1^T$. In practice (and somewhat confusingly), $A'$ is now referred to as $A$
    \item Repeat the algorithm with the updated $A$, and the next row/column. Observe each $l_i, u_i^T$ constructed are the rows/columns of the lower and upper triangular matrices of $L, U$ respectively.
\end{itemize}

% GAUSS TRANSFORMATIONS
\subsubsection{Gauss transforms}
\textbf{Guass transformation matrices} are linear transformations that zero out all entries below a certain entry (this is another way to think about the LU factorization). The columns of a Gauss transformation look like the values of $l_i$, where nonzero entries are divided by a pivot entry.

To compute $A=LU$, consider $L^{-1}A = U$, with $L^{-1}$ that "zeros-out" the columns of $A$ to get $U$. Call $L^{-1}, G$. As with the iterative algorithm above, we can multiply $A$ by iterative $G_i$'s to get $U$:

\begin{align*}
    L^{-1}A &= G_nG_{n-1}\dots G_2G_1A = U\\
    A &= G_1^{-1}\dots G_n^{-1}U = LU
\end{align*}


% PIVOTING
\subsection{Pivoting}
\subsubsection{When pivoting is needed}
Notice that this algorithm relies on the pivots, $a_{kk}$, being nonzero. It turns out this will occur if none of the $k \times k$ blocks of $A, \; A[1:k, 1:k],$ have a determinant of 0. \textbf{Proof by induction}:\\
\textit{Case k=1:} 
\begin{align*}
    A_1 &= L_1U_1 \longleftrightarrow det(A_1) = det(L_1U_1) \longleftrightarrow det(A_1) = det(L_1)det(U_1) \textrm{, by property of determinants}\\
    det(A_1) &= det(U_1) \textrm{, since determinant of a triangular matrix is a product of the diagonals and the diagonal of $L_1$ are 1's}\\
    det(A_1) &= a_{11} = u_{11} \rightarrow \textrm{so when determinant is not zero, we have a nonzero pivot}
\end{align*}
\textit{Case k=n:} assumed to be true\\
\textit{Case k=n+1:}
\begin{align*}
    A_1 &= L_1U_1 \longleftrightarrow det(A_{k+1}) = det(L_{k+1}U_{k+1}) \longleftrightarrow det(A_{k+1}) = det(L_{k+1})det(U_{k+1})\\
    det(A_{k+1}) &= det(U_{k+1}) \longleftrightarrow det(A_{k+1}) = u_{11}*u_{22}*\dots*u_{kk}\\
    & \textrm{but we know $u_{ii} \neq 0$ for $i \leq k$ from induction step, so when determinant is not zero,} \\ 
    & \textrm{we have pivot, $a_{k+1, k+1}$ nonzero}
\end{align*}
What's more, if the entries of $L$ are large (which occurs when entries in $A$ are really small and land on the pivot locations), then because of roundoff errors in a computer, this algorithm can generate errors. The \textbf{key} is to not have small values in the diagonal! Consider $A \in \mathbb{R}^{2\times 2}$ below. The issue arises when we need to calculate $\epsilon^{-1} + (\pi - \epsilon^{-1})$. With finite precision and $\epsilon$ small, this value is very different from $\pi$:

\begin{align*}
    A &= \begin{bmatrix} \epsilon & 1\\ 1 &  \pi \end{bmatrix},
    L = \begin{bmatrix} 1 & 0\\ \epsilon^{-1}& 1 \end{bmatrix},
    U = \begin{bmatrix} \epsilon & 1\\ 0 &  \pi - \epsilon^{-1} \end{bmatrix},
\end{align*}


% PIVOTING ALGORITHMS
\subsubsection{Pivoting algorithms}
Pivoting algorithms pivot the iterative version of $A$ to avoid the numerical issues identified above
\begin{itemize}
    \item \textbf{Partial/Row pivoting} performs row swaps at each step in the LU factorization so that the largest entry in a column appears in the pivot location. And we solve $PA = LU$, with P being a matrix storing the successive row swaps of $A$
    \item \textbf{Full pivoting} performs row and column swaps at each step in the LU factorization so that at each step, the largest remaining entry appears in the next pivot location. Here we solve $PAQ^T = LU$, with $P$ swapping rows of $A$, and $Q^T$ swapping columns.
    
    Full pivoting is \textbf{rank-revealing} since once the rank of the matrix $r$ iterations have been performed, the remaining block will contain only zeros and the algorithm can stop early (plus we learned something about the rank of $A$!
    \item \textbf{Rook pivoting} performs row and column swaps at each step in the LU algorithm, but instead of swapping the pivot for the largest remaining entry, it swaps the next pivot for the first entry encountered that is maximum in its row and column. This pivoting approach is also rank-revealing and computationally less expensive!
\end{itemize}


% CHOLESKY FACTORIZATION
\subsection{Cholesky factorization}
The Cholesky factorization is an LU factorization for Symmetric Positive Definite (SPD) matrices, where SPD matrix, $A = GG^T$, with $G$ lower triangular.\\
\textbf{Intuition:} An SPD matrix, $A$, can be written of the form

\begin{align*}
    A &= \begin{bmatrix} a & C^T\\ C &  B \end{bmatrix} \textrm{where a is 1x1, C is n-1x1, and b is n-1xn-1}
\end{align*}
After the first step of the LU factorization, we have the following matrix product, $A = L_1U_1$

\begin{align*}
    \begin{bmatrix} a & C^T\\ C &  B \end{bmatrix} &= 
    \begin{bmatrix} 1 & 0\\ C/a &  I \end{bmatrix} 
    \begin{bmatrix} a & C^T\\ 0 & B - (1/a)CC^T \end{bmatrix}
\end{align*}
Notice since $A$ is symmetric, $B$ is also symmetric, so $B - (1/a)CC^T$ must by symmetric by construction. We are also guaranteed to have the pivot, $a$ in entry $(1,1)$ of $A$, to be strictly greater than zero since $A$ is SPD: $a = e_1^TAe_1 > 0$. Next, we can further decompose the second matrix to

\begin{align*}
    A &= \begin{bmatrix} 1 & 0\\ C/a &  I\end{bmatrix}
    \begin{bmatrix} a & 0\\ 0 & B - (1/a)CC^T \end{bmatrix}
    \begin{bmatrix} 1 & C^T/a\\ 0 & I \end{bmatrix}
\end{align*}

\noindent Using the fact that $A$ SPD $\Rightarrow B^TAB$ SPD for $B$ nonsingular, observe that matrix $\begin{bmatrix} 1 & 0\\ C/a &  I\end{bmatrix}$ is nonsingular so therefore the matrix $\begin{bmatrix} a & 0\\ 0 & B - (1/a)CC^T \end{bmatrix}$ must be SPD. Which also means the submatrix $B - (1/a)CC^T$ is SPD. We can use induction to prove that the Cholesky factorization exists.
\\ \\
Continuing with this factorization, we get an equation of the form $A = LDL^T$ for $D$, diagonal, and $L$, lower triangular. It's common to rewrite $A = LDL^T$ in the form $A = GG^T$, where $G = LD^{\frac{1}{2}}$

\subsubsection{Cholesky factorization is unique}
By contradiction, suppose $A = GG^T = MM^T$ for $G \neq M$. We know $G, M$ nonsingular (consider $det(A)$) so
\begin{align*}
    GG^T &= MM^T\\
    I &= G^{-1}MM^TG^{-T} = (G^{-1}M)(G^{-1}M)^T \textrm{, since} (A^{-1})^T = (A^T)^{-1}\\
    (G^{-1}M)^{-T} &= (G^{-1}M)\\ \Rightarrow G^{-1}M & \textrm{ diagonal since } G^{-1}M \textrm{ lower triangular and } (G^{-1}M)^{-T} \textrm{ upper triangular}\\
    \Rightarrow G^{-1}M &= D \Rightarrow M = GD\\
    I &= (G^{-1}GD)(G^{-1}GD)^T = DD^T = D^2 \Rightarrow \textrm{ so the entries of D are on the order of 1}
\end{align*}

% SCHUR COMPLEMENT
\subsection{Schur complement}
A useful way to think about the LU factorization is with the \textbf{Schur complement} matrix structure. First observe $A$ can be written in the following form
\begin{equation*}
    A = \begin{bmatrix} A_{11} & A_{12}\\ A_{21} & A_{22}\end{bmatrix}
\end{equation*}
If we run the LU factorization algorithm for $k$ steps, the resulting $A' = A$ is equal to 
\begin{equation*}
    A = \begin{bmatrix} I & 0\\ A_{21}A_{11}^{-1} & I\end{bmatrix}
    \begin{bmatrix} A_{11} & 0\\ 0 & A_{22} - A_{21}A_{11}^{-1}A_{12}\end{bmatrix}
    \begin{bmatrix} I & A_{21}A_{11}^{-1}\\ 0 & I\end{bmatrix}
\end{equation*}
The bottom-right block of $A'=A, A_{22}' = A_{22}$ is equal to $A_{22} - A_{21}A_{11}^{-1}A_{12}$ from the original matrix. This is called the \textbf{Schur complement} of $A$

\subsubsection{Schur complement derivation}
At any step in the LU factorization, $A$ can be written in the form
\begin{equation*}
    A = \begin{bmatrix} A_{11} & A_{12}\\ A_{21} & A_{22}\end{bmatrix} = \begin{bmatrix} L_{11} & 0\\ L_{21} & L_{22}\end{bmatrix}
    \begin{bmatrix} U_{11} & U_{12}\\ 0 & U_{22}\end{bmatrix}
\end{equation*}
From this equality, we can create a system of equations and derive

\begin{align*}
    U_{12} &= L_{11}^{-1}A_{12}\\
    L_{11}^{-1} &= L_{21}A_{21}A_{11}^{-1}\\
    A_{22} - L_{21}U_{12} &= L_{22}U_{22}\\
    A_{22} - A_{21}A_{11}^{-1}A_{12} &= L_{22}U_{22}
\end{align*}
Notice that the Schur complement equals the product of $L_{22}U_{22}$. The next step in the derivation is to show that $A_{22}'$ in the LU factorization is equal to $A_{22} - L_{21}U_{12}$ since at each step we're subtracting $l_iU_i^T$, which can be stored as the nonzero rows/columns of $L_{21}U_{12}$. So

\begin{align*}
    A_{22}' &= A_{22} - L_{21}U_{12} \\
    &= (L_{21}U_{12} + L_{22}U_{22}) - L_{21}U_{12}\\
    &= L_{22}U_{22}\\
    &= A_{22} - A_{21}A_{11}^{-1}A_{12}
\end{align*}

% QR FACTORIZATION
\section{QR factorization}
The QR factorization decomposes a matrix, $A \in \mathbb{R}^{m \times n}, m \geq n$ into an orthogonal (orthonormal) matrix, $Q$, and an upper triangular matrix, $R$. When $A \in \mathbb{C}^{m \times n}$, $Q$ is unitary.\\ \\
Recall for $Q \in \mathbb{R}$, orthogonal, $Q^TQ = I$; for $Q \in \mathbb{C}$, unitary, $Q^HQ = I$; $\norm{Qx}{2} = \norm{x}{2}$\\ \\
If $A$ is skinny (i.e., $n << m$), $QR$ can take two different forms. $Q \in \mathbb{R}^{m \times m}$ can be square and $R \in \mathbb{R}^{m \times n}$ can be skinny. Or $Q \in \mathbb{R}^{m \times n}$ can be skinny and $R \in \mathbb{R}^{n \times n}$ can be square.

% UNIQUENESS
\subsection{The QR factorization is unique}
\textbf{Proof} that the QR factorization is unique for full rank matrix, $A$:
\begin{align*}
    A &= QR \longleftrightarrow Q^TA = R \longleftrightarrow ^TQ^TA = R^TR \longleftrightarrow (QR)^TA = R^TR \longleftrightarrow A^TA = R^TR
\end{align*}
We now have a matrix, $A^TA$ that can be written of the form $R^TR$, which is the structure of the Cholesky factorization. Suffice to show that $A^TA$ is Symmetric and Positive Definite (SPD) to prove the uniqueness of R.\\ \\
Since $A$ is full rank, it follows that $Q$ is also unique (since $AR^{-1} = Q$). $A^TA$ SPD:
\begin{align*}
    \textrm{Symmetric: }& (A^TA)^T = A^TA\\
    \textrm{Positive definite: for }& x\neq 0,\\
    & x^TA^TAx = (Ax)^T(Ax) =(QRx)^T(QRx) = x^TR^TQ^TQRx =(Rx)^T(Rx)\\
    & \textrm{Rx is of the form } Rx = \begin{bmatrix} r_{11}x_1 \\ r_{12}x_1 + r_{22}x_2 \\ \vdots \\ \sum_{i=1}^n r_{in}x_i\end{bmatrix} \textrm{, so } (Rx)^T(Rx) = \sum_{i=1}^n (\sum_{j \leq i} r_{ij}x_j)^2\\
    &\textrm{So, } (Rx)^T(Rx) >0 \textrm{ for } x \neq 0
\end{align*}

% HOUSEHOLDER REFLECTION
\subsection{Householder reflection}
The Householder reflection is a QR factorization algorithm. It relies on the principles of reflection matrices.
\subsubsection{Householder reflection algorithm}
\begin{itemize}
    \item Construct $Q^T$ for each column in $A$ that projects it onto a corresponding column of an upper right triangular matrix, $R$.
    \item E.g., for first column $a_1$: Want $Q_1^T$ such that $Q_1^Ta_1 = r_1$, where $r_1 = \pm \norm{a_1}{2}e_1$ (since $Q^T$ is orthogonal). This equates to finding $Q_1^T$ that reflects $a_1$ onto $e_1$
    \item \textbf{The key} to the iterative part of the algorithm is to construct $Q_i^T, i > 1$ with an identity matrix in the upper-left $i-1 \times i-1$ quadrant, and a smaller $Q_i^{*T}$ in the lower right $n-i \times n-i$ quadrant, filling the remaining sections of the matrix with $0$'s.
\end{itemize}

\subsubsection{Constructing the Householder reflection permutation}
\noindent The \textbf{Householder reflection} maps $a \rightarrow \norm{a}{2}e_1$ with
\begin{equation*}
    P = I - \beta vv^T \textrm{, where $v = a - \norm{a}{2}e_1$, and $\beta = 2/v^Tv$}
\end{equation*}
\begin{itemize}
    \item Mechanics: multiplying $Px$ is the same as taking the vector $x$ and subtracting $\frac{2vv^T}{v^Tv}x$ from it, twice the projection of $x$ onto $v$ (this is reflection)
    \item Householder: In our case we want to reflect $a$ onto $\norm{a}{2}e_1$.  $a + \norm{a}{2}e_1$ is the line of reflection, and $a - \norm{a}{2}e_1$, perpendicular to this, is the vector that defines the line of reflection
    \item In cases where the other entries in $a$ are much smaller than $a_1$, it may be advantageous to project onto $-\norm{a}{2}e_1$ instead of $-\norm{a}{2}e_1$ (to avoid roundoff errors. In this case, we choose $v = a + \norm{a}{2}e_1$.
\end{itemize}

\noindent \textbf{Aside:} The fixed points of a reflection, $P$, remain unchanged when multiplied by the reflection, $Px=x$. Geometrically, these are the points that are \textit{orthogonal} to the vector $v$ defining the reflection (i.e., $v^Tx=0$)


% GIVENS TRANSFORMATION
\subsection{Givens transformation}
While the Householder reflection is useful for operating on dense matrices, if we are presented with a sparse matrix, the sequential reflections of will transform a sparse matrix into a dense matrix and create unnecessary complexity. For example, consider upper Hessenberg matrix, $H$. the \textbf{Givens transformation} will allow us to zero our the subdiagonal with less complexity!

\subsubsection{Givens transformation algorithm}
A \textbf{Givens rotation} rotates $u = (u_1, u_2)^T$ to $\norm{u}{2}e_1$. The matrix that does this, $G^T$, is defined by
\begin{equation*}
    G^T = \begin{bmatrix} c & -s \\ s & c \end{bmatrix}, c = \frac{u_1}{\norm{u}{2}}, s = -\frac{u_2}{\norm{u}{2}}
\end{equation*}
A full matrix, $P_i$, can be constructed to only contain this targeted transformation. Sequentially, the $P_i$'s can multiply $A$ to arrive at $R$

% GRAM-SCHMIDT TRANSFORMATION
\subsection{Gram-Schmidt transformation}
Householder and Givens transformations produce square $Q \in \mathbb{R}^{n\times n}$ matrices. If $A \in \mathbb{R}^{m\times n}$ is tall and thin, then we may want a method to create a tall and thin $Q \in \mathbb{R}^{m\times n}$. The Gram Schmidt transformation does this.\\ \\
Similar to the $LU$ factorization, the \textbf{Gram-Schmidt Transformation} starts with the property that $A=QR$ can be written as a sum of the outer products of the columns of $Q$ and rows of $R$: $A = QR = q_1r_1^T + \dots q_mr_m^T$:
\begin{align*}
    r_{11} &= \norm{a_1}{2} \textrm{, since } \norm{a_1}{2} = \norm{q_1r_{11}}{2} \textrm{ and $q_i$ orthogonal}\\
    q_1 &= \frac{1}{r_{11}}a_1\textrm{, since } a_1 = q_1r_{11} \textrm{ by construction of } QR \\
    r_{1j} &= q_1^Ta_j \textrm{, (repeat for all $j$) since }\\
    & a_j = q_1r_{1j} + \dots + q_jr_{jj}\\
    & q_1^Ta_j = q_1^Tq_1r_{1j} + \dots + q_1^Tq_jr_{jj}\\
    & q_1^Ta_j = r_{1j} \textrm{, since $q_i$ orthonormal}\\
    A' &= A - q_1r_1^T
\end{align*}
Repeat for $A'$, the construction of $r_{kk}, q_k, r_{kj}$
\begin{align*}
    a_k &= \sum_{i=1}^kr_{ik}q_i = r_{kk}q_k + \sum_{i=1}^{k-1}r_{ik}q_i\\
    1. \; &r_{ik} = q_i^Ta_k \textrm{ for each $r_{ik}, i < k$, since $Q$ orthonormal and $q_{k-1}$ known}\\
    2. \; &z = r_{kk}q_k = q_k - \sum_{i=1}^{k-1}r_{ik}q_i\\
    3. \; & r_{kk} = \norm{z}{2}, \; q_k = \frac{z}{r_{kk}}
\end{align*}


% LEAST SQUARES
\subsection{QR factorization to solve least-squares problems}
When $A$ is tall and thin, it is unlikely that we get a solution to $Ax = b$. Instead, we choose to solve the least-squares problem, $argmin_x\norm{Ax - b}{2}$. 

% NORMAL EQUATIONS
\subsubsection{Method of normal equations}
Assuming $A$ full rank. Geometrically, the point, $x$ which solves $argmin_x\norm{Ax - b}{2}$ is one where $b-Ax$ is orthogonal to the range of $A$. To solve for this:
\begin{align*}
    \textrm{Want: } (b-Ax) &\perp \{z \vert z = Ay\} \longleftrightarrow (b-Ax) \perp range(A) \longleftrightarrow (b-Ax) \perp a_i, \forall i \in A\\
    a_1^T(b-Ax) &= 0, \forall i \in A \longleftrightarrow A^T(b-Ax) = 0 \longleftrightarrow x = (A^TA)^{-1}A^Tb
\end{align*}
We can use Cholesky fast/accurate solve since $A^TA$ is SPD. Notice, condition number of $A^TA, \kappa(A^TA) = \kappa(A)^2$, so if A is poorly conditioned, this method can get inaccurate. 

% QR METHOD FOR LEAST SQUARES
\subsubsection{QR method for least squares}
Assuming $A$ full rank. The QR method for least squares attempts to address the issue of poor conditioning and may also lead to faster computation. We construct the QR method for least squares with one of the normal equation equalities:

\begin{align*}
    A^T(Ax-b) &= 0 \longleftrightarrow R^TQ^T(Ax-b) = 0\\
    Q^T(Ax-b) &= 0 \textrm{, since we assume $A, R$ full rank (multiply both sides by $R^{-T}$})\\
    Q^TQRx-Q^Tb &= 0 \longleftrightarrow Rx = Q^Tb \longleftrightarrow x = R^{-1}Q^Tb
\end{align*}

% SVD FOR RANK-DEFICIENT A
\subsubsection{SVD for rank-deficient A}
When $A$ not full rank, we can get infinite solutions (a line of points that satisfy $argmin_x\norm{Ax - b}{2}$). To choose $x$, we add constraint $\min_x \norm{x}{2}$ to our original objective function, $argmin_x\norm{Ax - b}{2}$.\\ \\
We can use the "thin" version of the Singular Value Decomposition to solve this, with $A \in \mathbb{R}^{m \times n}, \; rank(A) = r$, construct $U \in \mathbb{R}^{m \times r}, \; \Sigma \in \mathbb{R}^{r \times r}$(, notice this $\Sigma$ has an inverse), $V^T \in \mathbb{R}^{r \times n}$. And calculate $x$ as
\begin{align*}
    (Ax-b) &\perp range(A) \longleftrightarrow (Ax-b) \perp range(U) \textrm{, since $R(A) = R(U)$ for $A=U\Sigma V^T$}\\
    U^T(Ax-b) &= 0 \longleftrightarrow U^T(U\Sigma V^Tx - b) = 0 \longleftrightarrow \Sigma V^Tx = U^Tb\\ 
    x &= V\Sigma^{-1}U^Tb \textrm{ (the "thin" SVD here provides a nonsingular $\Sigma\in \mathbb{R}^{r \times r}$, so we can take the inverse}
\end{align*}
Observe for $\min_x \norm{x}{2}$ that the $x \perp N(A)$ is the shortest vector between $N(A)$ and the vector/plane of solutions to $argmin_x\norm{Ax - b}{2}$. This value must be in $R(V)$ since $R(V) = N(A)^\perp$


% ITERATIVE METHODS FOR EIGENVALUES
\section{Iterative methods to find eigenvalues}
\subsection{Power iteration}
Given $\lambda_1 > \lambda_2 \geq \dots \geq \lambda_n \in \lambda(A)$, the \textbf{Power iteration} is a process for finding $\lambda_1$. The basic idea is to repeatedly multiply matrix $A$ times a vector (normalizing each time) and eventually the first eigenvalue and eigenvector will emerge. This process assumes $A$ is diagonalizable, meaning it can be written of the form $A = X\Lambda X^{-1}$. Reminder: for $A$ diagonalizable $\Longrightarrow A^k = X \Lambda^k X^{-1}$
\begin{align*}
    A^k &= \sum_{i = 1}^n \lambda_i^k x_i y_i^T \textrm{ where $Y = X^{-1}$}\\
    A^k &\approx \lambda_1^kx_1y_1^T \textrm{ since } \lambda_1 > \lambda_2\\
    A^kq &\approx \lambda_1^kx_1y_1^Tq = \lambda_1^k(y_1^Tq)x_1 \textrm{, since $y_1^Tq$ is a scalar. Observe } A^kq \parallel x_1
\end{align*}
This theory is implemented in practice with the following formula
\begin{align*}
    1. \;& q_0 \textrm{, vector chosen at random}\\
    2. \;& z_k = Aq_k = A^kq_0 \textrm{, evaluating for convergence if } z_k \parallel q_k \rightarrow z_k^Tx_k = \norm{z}{2}\norm{x}{2}\\
    3. \;& q_{k+1} = \frac{z_k}{\norm{z_k}{2}} = \frac{A^kq_0}{\norm{A^kq_0}{2}} \approx (\frac{\lambda_2}{\abs{\lambda_1}})^kx_1
\end{align*}
Since $A^{k}q_0 = Aq_{k} \approx \lambda_1 x_1$, where $\norm{x_1}{2} = 1 \; (WLOG)$ and $q_k \parallel x_1$, we can solve for $\lambda$:

\begin{align*}
    Aq_k \approx \lambda_1 x_1 &\Longrightarrow Ax_1 \approx \lambda_1x_1 \Rightarrow x_1^HAx_1 \approx \lambda_1\\
    \textrm{Convergence: } &O((\abs{\frac{\lambda_1}{\lambda_2}})^K) \textrm{, since}\\
    A^kq_0 &= \sum_i \alpha_i A^k x_i = \sum_1 \alpha_i \lambda_i^k x_i\\
    &= \alpha_1\lambda_1^k(x_i + \frac{\alpha_2}{\alpha_1}(\frac{\lambda_2}{\lambda_1})^k + \dots + \frac{\alpha_n}{\alpha_1}(\frac{\lambda_n}{\lambda_1})^k) \\
    \Longrightarrow \norm{A^kq_0}{2} &= \abs{\alpha_1\lambda_1^k}(1 + O(\frac{\lambda_2}{\lambda_1})^k) 
\end{align*}
\textbf{Convergence:} $O(\abs{\frac{\lambda_2}{\lambda_1}}^k)$


\subsection{Inverse iteration}
This process finds the eigenvector (and corresponding eigenvalue) of $A$ that is closest to the value $\mu$. The basic idea is to multiply matrix $(A - \mu I)^{-1}$ iteratively by a random vector, $z$, normalizing each time. Eventually, you will get the eigenvector for the eigenvalue closest to $\mu$. \\ \\ 
Observe $(A - \mu I)^{-1}$ has the same eigenvectors of $A$:
\begin{align*}
    (A - \mu I)^{-1}x &= \lambda x \longleftrightarrow x = (A - \mu I)x = \lambda Ax - \lambda \mu x \longleftrightarrow \lambda Ax = x + \lambda \mu x \longleftrightarrow Ax = \frac{(1 + \lambda \mu)}{\lambda}x
\end{align*}
Performing the power iteration on $(A - \mu I)^{-1}$, the largest eigenvalue to emerge will be of the form $\frac{1}{\lambda_i - \mu}$, and we get
\begin{align*}
    (A - \mu I)^{-1k}q_0 &= (A - \mu I)^{-1}q_{k} \approx \lambda_i x_i \textrm{, where } \norm{x_i}{2} = 1 \; (WLOG) \textrm{ and } q_k \parallel x_i
\end{align*}
Since $x_i$ is also an eigenvalue of $A$, we can solve $x_i^HAx_i = \lambda_i$ for the $\lambda_i$ closest in magnitude to $\mu$. \textbf{Convergence: } $O(\abs{\frac{\lambda_i - \mu}{\lambda_j - \mu}})^k)$, where $\lambda_j$ is the next closest eigenvalue to $\mu$

\subsection{Eigenvalues of similar matrices}
\textbf{Theorem:} For $S$ nonsingular and $A = S^{-1}BS$, then i) $\lambda(A) = \lambda(B)$ and ii) $x$ eigenvector of $A$ $\Leftrightarrow S^{-1}x$ eigenvector of $B$.
\begin{align*}
    i) \; &\lambda(A) = \lambda(B):\\
    &det(A - \lambda I) = det(S^{-1})det(A - \lambda I)det(S) = det(S^{-1}(A - \lambda I)S) = det(B - \lambda I)\\
    ii) \;& \textrm{$x$ eigenvector of $A \Leftrightarrow S^{-1}x$ eigenvector of $B$}\\
    &Ax = \lambda x \rightarrow S^{-1}Ax = \lambda S^{-1}x \rightarrow S^{-1}ASS^{-1}x = \lambda S^{-1}x \rightarrow B(S^{-1}x) = \lambda (S^{-1}x)
\end{align*}

\subsection{Eigenvalues from invariant subspaces}
\textbf{Theorem:} $X \in \mathbb{R}^{n \times m}$ is an invariant subspace of $A \in \mathbb{R}^{n \times n} \Leftrightarrow$ there is a $B \in \mathbb{R}^{n \times m}$ such that $AX = XB$. \textbf{Proof:}

\begin{align*}
    \Rightarrow: \;& X \textrm{ invariant} \longrightarrow Ax_i \in X \longrightarrow Ax_i = \sum_{j=1}^mx_jb_{ji} \longrightarrow AX = XB\\
    \Leftarrow: \;& AX = XB \longrightarrow Ax_i = \sum_{j=1}^mx_jb_{ji} \longrightarrow Ax_i \in X \longrightarrow X \textrm{ invariant}
\end{align*}
Furthermore, when $AX = XB$, the $m$ eigenvalues of $B$ are also eigenvalues of $A$: $By = \lambda y \longrightarrow XBy = \lambda Xy \longrightarrow AXy = \lambda Xy$

\subsection{Orthogonal iteration}
This process finds $r$ eigenvalues and eigenvectors of $A$ in a single iterative process.\\ \\
First, consider how to construct orthogonal columns to reveal subsequent eigenvalues. Assume we use power iteration to compute $q_1$. To get $q_2$:
\begin{align*}
    A^k &= \lambda_1x_1y_1^T + \lambda_2x_2y_2^T + \dots\\
    PA^k &= \lambda_1Px_1y_1^T + \lambda_2Px_2y_2^T + \dots \textrm{, where } P = I - x_1x_1^T\\
    PA^k &= 0 + \lambda_2Px_2y_2^T + \dots \textrm{, since } Px_1 = Ix_1 - x_1x_1^Tx_1 = x_1 - x_1 = 0\\
    PA &\textrm{ can now be used to apply the power iteration to to reveal $\lambda_2$ and } (I - x_1^Tx_1)x_2
\end{align*}
The general process is:
\begin{itemize}
    \item Start with $\lambda_1, q_1$ from power iteration
    \item Build $P_2$, orthogonal projector onto $\{q_1\}^\perp$, use power iteration to reveal ($\lambda_2, q_2$)
    \item Build $P_r$, orthogonal projector onto  $\{q_1, \dots, q_{r-1}\}^\perp$, use power iteration to reveal ($\lambda_r, q_r$)
\end{itemize}
Now consider the QR decomposition of $X$, observing its connection to the Schur Decomposition:
\begin{align*}
    A &= X\Lambda X^{-1} = QR \Lambda R^{-1} Q^H = QTQ^H \textrm{, where upper triangular } T = R\Lambda R^{-1}
\end{align*}
\begin{itemize}
    \item The eigenvalues of $A$ are on the diagonal of $T$
    \item By construction, each column of $Q$ is projecting the corresponding column of $X$ onto a vector orthogonal to the preceding ones
    \item The span of the columns of $Q, span\{q_1, \dots, q_n\}$ will be equal to the span of the columns of $X, span\{x_1, \dots, x_n\}$.
\end{itemize}
The process for the \textbf{orthogonal iteration} is:
\begin{align*}
    1. \;& AQ_k \rightarrow Z \textrm{, where $k$ is the iteration and } Q_0 = I\\
    2. \;& Z \rightarrow Q_{k+1}R_{k+1} \textrm{, the QR factorization of $Z$}\\
    3. \;& \textrm{Repeat } AQ_{k+1} \rightarrow Z \textrm{ and eventually } Q_k \rightarrow Q
\end{align*}
Note in each iteration we are calculating $Q_{k+1}^HAQ_k = R_{k+1}$

\subsubsection{Reveal eigenvectors of $A$ from $T$}
Motivation: $A = X\Lambda X^{-1}$ can be hard to calculate.
\begin{align*}
    A &= X\Lambda X^{-1} = QR \Lambda R^{-1}Q^H = QTQ^H \textrm{, where } T = R\Lambda R^{-1}\\
    A &= QY\Lambda Y^{-1}Q^H \textrm{, where $T = Y \Lambda Y^{-1}$ is easier to compute}
\end{align*}
Focusing on $T = Y \Lambda Y^{-1}$, choose some $\lambda_i$ (we could get from power or QR iteration). 
\begin{align*}
    Tx &= \lambda_i x\\
    (T - \lambda_i I)x &= 0\\
    (T - \lambda_i I)x  &= \begin{bmatrix} T_{11} - \lambda_i I & T_{12} & T_{13}\\ 0 & 0 & T_{23}\\
            0 & 0 & T_{33} - \lambda_i I \end{bmatrix} \begin{bmatrix} X_1 \\ X_2 \\ X_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \textrm{, where one diagonal element is 0}\\
    \textrm{And solve with back substitution:}&\\
            X_3 &= 0: (T_{33} - \lambda_i I) X_3 = 0\\
            X_2 &\textrm{ is a free parameter} \in \mathbb{R}: 0X_2 + T_{33}X_3 = 0 \Longrightarrow 0X_2 = 0\\
            X_1 &= -(T_{11}-\lambda_i I)^{-1}T_{12}X_2: (T_{11}-\lambda_i I)X_1 + T_{12}X_2 + T_{13}X_3 = 0
\end{align*}
So, if $T$ upper triangular with $\lambda_i$ on diagonal of T, you can figure out all the columns of $Y$ for $T = Y\Lambda Y^{-1}$. It follows the eigenvectors of $A$ are $Qy_i$. Note, $(T_{11} - \lambda_i I)$ nonsingular as long as the algebraic multiplicity of $\lambda_i$ is 1.


\subsubsection{Rate of convergence in orthogonal (and QR) iteration}
\textbf{Property:} the angle between two subspaces, $U$ and $V$, is defined as $\norm{UU^T - VV^T}{2}$\\
\noindent In orthogonal interation, the span of those $i$ columns of $Q_k, span\{q_1, \cdots, q_i\} \longrightarrow$ the span of those columns of $X, span \{x_1, \cdots, x_i\}$. Convergence is dictated by how quickly these spans converge. The rate of convergence is $O(\abs{\frac{\lambda_{i+1}}{\lambda_i}}^k)$. \textbf{Note:} difficulties arrise when $\abs{\frac{\lambda_{i+1}}{\lambda_i}}$ is close to 1.

% QR ITERATION
\subsection{QR iteration}
The QR iteration builds directly on the framework of the orthogonal iteration. In orthogonal iteration, we compute $T_{k+1}$ with the eigenvalues of $A$ appearing on the diagonal of $T_{k+1}$
\begin{equation*}
    Q_{k+1}^HAQ_k = T_{k+1} \textrm{ with }AQ_k = Z = Q_{k+1}T_{k+1}
\end{equation*} 
In the QR iteration, we ask if we can go from $T_k$ to $T_{k+1}$ directly. Observe
\begin{align*}
    A &= Q_k T_k Q_k^H \Longrightarrow T_k = Q_k^HAQ_k\\
    AQ_k &= Q_{k+1}R_{k+1} \Longrightarrow Q_{k+1}^HA = R_{k+1}Q_k^H\\ \\
    T_k &= Q_k^H(Q_{k+1}R_{k+1}) \longrightarrow T_k = U_{k+1} R_{k+1} \textrm{ for } U_{k+1} = Q_k^HQ_{k+1}\\
    T_{k+1} &= (R_{k+1}Q_k^H)Q_{k+1} \longrightarrow T_{k+1} = R_{k+1}U_{k+1} \textrm{ for } U_{k+1} = Q_k^HQ_{k+1}
\end{align*} 
So we have an algorithm for $T_k \rightarrow T_{k+1}$, this process is the \textbf{QR iteration}:
\begin{align*}
    1. \;& T_k \longrightarrow U_{k+1}R_{k+1} \textrm{, the QR factorization of } T_k\\
    2. \;&R_{k+1}U_{k+1} \longrightarrow T_{k+1}\\
    3. \;&\textrm{Repeat with } T_{k+1}
\end{align*}
\textbf{Proof by induction:} $R_{k+1}$ is the same in both QR factorization of $A = Q_{k+1}R_{k+1}$ and $T_k = U_{k+1}R_{k+1}$
\begin{align*}
    case \; 1:&\\
    A &= AQ_0 = Q_1R_1, A = T_0 =U_1R_1^* \textrm{, and } T_1 = Q_k^HAQ_1\\
    U_1R_1^* &= Q_0^TQ_1R_1 = Q_1R_1 \; \Longrightarrow R_1^* = R_1 \textrm{ and } U_1 = Q_0^TQ_1\\
    case \; k:& \textrm{ Assume } R_k^* = R_k, U_k = Q_{k-1}^TQ_k \textrm{, and } T_k = Q_k^HAQ_k\\
    case \; k+1:&\\
    AQ_k &= Q_{k+1}R_{k+1}\\
    T_k &=U_{k+1}R_{k+1}^* = Q_k^HAQ_k = Q_k^HQ_{k+1}R_{k+1} \Longrightarrow R_{k+1}^* = R_{k+1} \text{ and } U_{k+1} = Q_k^HQ_{k+1}\\
    T_{k+1} &= R_{k+1}U_{k+1} = Q_{k+1}^H(Q_{k+1}R_{k+1})U_{k+1} = Q_{k+1}^HAQ_kQ_k^HQ_{k+1} \Longrightarrow T_{k+1} = Q_{k+1}^HAQ_{k+1}
\end{align*}

\subsection{QR iteration on upper Hessenberg}
Each QR iteration step of a dense matrix is $O(n^3)$. If we run for $O(k)$ iterations, then this algorithm is $O(kn^3)$. To reduce flops, we can first convert $A$ to upper Hessenberg ($H = Q^HAQ$) with $O(n^3)$, and proceed with QR iteration on $H$ using Givens rotations with complexity $O(n^2)$ (so overall complexity is reduced to $O(n^3 + kn^2)$):
\begin{align*}
    \textrm{Choose } Q_1^T &= \begin{bmatrix} 1 & 0 \\ 0 & \tilde{P_1} \end{bmatrix} \textrm{ to perform a Householder rotation onto the first two entries of } a_1 \in A\\
    \textrm{Observe }Q_1^T A Q_1 &= \begin{bmatrix} 1 & 0 \\ 0 & \tilde{P_1} \end{bmatrix} A \begin{bmatrix} 1 & 0 \\ 0 & \tilde{P_1^T} \end{bmatrix} = \begin{bmatrix} x & x & \cdots & \\
        x & x & \cdots \\ 0 & x &\cdots \\ \vdots & \vdots & \ddots \end{bmatrix} \textrm{ where $a_{11}$ is never changed, the rest of $a_1$}\\
        &\textrm{is only operated on by $\tilde{P_1}$, and the rest of $a_1^T$ is only operated on by $\tilde{P_1^T}$}\\
    \textrm{Continuing on, } & Q_n^T \dots Q_2^TQ_1^T A Q_1Q_2 \dots Q_n = H = Q^HAQ \textrm{ where } Q_k^T = \begin{bmatrix} I_k & 0 \\ 0 & \tilde{P_k} \end{bmatrix}
\end{align*}
\textbf{$H$ remains upper Hessenberg in QR iteration:} This follows since in the first step of QR iteration, $H_k$ is transformed to $R_k$ with givens rotations, $U_k^HH_k = R_k$. And in the second step of QR iteration, $H_{k+1}$ is created as $R_kU_k = H_{k+1} = U_k^HH_kU_k$. Since $U_k$ is a series of givens rotations, these rotations can be constructed/ordered so that $H_{k+1}$ preserves upper Hessenberg.

\subsection{QR iteration with shift}
When $\lambda_{i+1}$ is close to $\lambda_i$, \textbf{QR iteration with shift} accelerates convergence. First observe for $\lambda_i \in \lambda(A) \rightarrow (\lambda_i - \mu) \in \lambda(A - \mu I)$. In this algorithm, at each step we shift $T_k$ by $\mu I$. For $\mu$ close to $\lambda_{i+1}$ close to $\lambda_i$, the resulting converence, $\abs{[(\lambda_{i+1} - \mu) / (\lambda_i - \mu)]}^k$ will be faster. Shift does not require that $\abs{\lambda_1} > \abs{\lambda_2} \geq \dots \geq \abs{\lambda_n}$.\\ \\
In general, the \textbf{QR iteration with shift} works by shifting the last eigenvalue (smallest in absolute value), updating the shift in each iteration. The last eigenvalue makes sense here because it preserves the eigenvalue ordering:
\begin{align*}
    1. \;& \mu_k = T_k[n, n]\\
    2. \;& (T_k - \mu_k I) \longrightarrow U_{k+1}R_{k+1} \textrm{, QR factorization of the shifted } T_k\\
    3. \;& R_kU_k + \mu_k I \longrightarrow T_{k+1} \textrm{, and repeat!}\\
\end{align*}
Observe, this shift preserves the original QR iteration:
\begin{align*}
    (T_k - \mu_ I) &= U_{k+1}R_{k+1} \Longrightarrow U_{k+1}^HT_k - \mu_k U_{k+1}^H = R_{k+1}\\
    T_{k+1} &= R_{k+1}U_{k+1} + \mu_k I \Longrightarrow T_{k+1} = (U_{k+1}^HT_k - \mu_k U_{k+1}^H)U_{k+1} + \mu_k I\\
    T_{k+1} &= U_{k+1}^HT_kU_{k+1} - \mu_k I + \mu_k I = U_{k+1}^HT_kU_{k+1}
\end{align*}


\subsubsection{Implicit Q theorem}
The \textbf{implicit Q theorem} tells us that if i) we get any upper Hessneberg, $H_{k+1}$ from a transformation of $H_k \rightarrow H_{k+1}$ of the form $U^TH_kU$ ii) $We_1 = Qe_1$ for two such transformations, then the columns of $W$ and $Q$ are equal, up to a sign.\\ \\
\textbf{Proof:} We show for $A = QHQ^T$, $Q$ orthogonal and $H$ upper Hessenberg, that $Q$, $H$ are determined by $A$ and $Qe_1$:
\begin{align*}
    AQ & = QH \textrm{, assume we know } q_1, \dots, q_k \textrm{ of } Q \\
    A \begin{bmatrix} Q_k & X \end{bmatrix} &= 
        \begin{bmatrix} Q_k & X \end{bmatrix} 
        \begin{bmatrix} H_k & X \\ 0 & X \end{bmatrix} \textrm{, $X$ unknown and } H_k \in \mathbb{R}^{k \times k}\\
    Aq_k &= \sum_{i=1}^kh_{i,k}q_i + k_{k+1,k}q_{k+1} \textrm{, the kth column of $AQ$, where } q_j^TAq_k = h_{j,k}\\
    k_{k+1,k}q_{k+1} &= Aq_k - \sum_{i=1}^kh_{i,k}q_i \textrm{, the RHS of which is known}\\
    &\Rightarrow \abs{h_{k+1,k}} = \norm{Aq_k - \sum_{i=1}^kh_{i,k}q_i}{2} \textrm{ and } q_{k+1} = \frac{Aq_k - \sum_{i=1}^kh_{i,k}q_i}{h_{k+1,k}}
\end{align*}
\textbf{Conclusion:} if we know the first k columns of $Q$, the subsequent column and elements of an upper Hessenberg matrix are determined up to a sign.

\subsubsection{Fracis shift}
The \textbf{Francis shift} is a way of selecting shifts based on the bottom-right $2\times 2$ block in a way that maintains a real-valued matrix. In effect, we double-shift using complex conjugates, $\mu, \overline{\mu}$:
\begin{align*}
    H_{k-1} - \mu I &= U_kR_k\\
    H_k &= R_kU_k + \mu I\\
    H_k - \overline{\mu}I &= U_{k+1}R_{k+1}\\
    H_{k+1} &= R_{k+1}U_{k+1} + \overline{\mu}I\\
    H_{k+1} &= U^H_{k+1}H_kU_{k+1} = U^H_{k+1}U^H_kH_{k-1}U_k1U_{k+1} = (U_kU_{k+1})^HH_{k-1}(U_kU_{k+1})
\end{align*}
\textbf{Proof} Consider QR factorization to show $(U_1U_2)$ is real
\begin{align*}
    (U_kU_{k+1})(R_{k+1}R_k) &= U_k(H_k - \overline{\mu}I)R_k = U_k(R_kU_k + \mu I - \overline{\mu}I)R_k = U_kR_k(U_kR_k + (\mu - \overline{\mu})I)\\
    &= (H_{k-1} - \mu I)(H_{k-1} - \mu I + (\mu - \overline{\mu})I) = (H_{k-1} - \overline{\mu}I)(H_{k-1} - \mu I)\\
    &= H_{k-1}^2 - (\mu + \overline{\mu})H_{k-1} + \abs{\mu}^2I \textrm{, where each component of the polynomial is} \in \mathbb{R}
\end{align*}
From uniqueness of QR factorization, $(U_1U_2)$ must be real as well. So at any step of the Francis shift, we want $H_{k+1} = Q^TH_{k-1}Q$
\begin{itemize}
    \item Define $M = H_{k-1}^2 - (\mu + \overline{\mu})H_{k-1} + \abs{\mu}^2I$, noticing $Me_1$ only has nonzero entries in the first three rows
    \item Want to build $V = U_1U_2$ to do shift, noticing we can get $V$ from QR factorization of $M = VR = (U_1U_2)(R_2R_1)$
    \item Using bulge chasing starting with $P_1^TMe_1 = e_1$, noticing i) for $M = (U_1U_2)(R_2R_1)$ that $U_1U_2$ also has only has nonzero entries in the first three rows, ii) by implicit Q theorem $V^THV = (U_1U_2)^TH(U_1U_2)$ is upper Hessenberg
\end{itemize}

\subsection{QR iteration with deflation}
\textbf{Deflation} allows us to break up the current QR iteration process into two smaller/easier problems.
\begin{itemize}
    \item If any sub-diagonal element of an upper Hessenberg matrix, $H$, is 0, it can be written as $H = \begin{bmatrix} H_{11} & H_{12} \\ 0 & H_{22} &\end{bmatrix}$ with $H_{11}$ and $H_{22}$ upper Hessenberg and $\lambda(H) = \lambda(H_{11})\cup \lambda(H_{22})$
    \item Therefore, if when updating $T_k = R_kU_k + \mu_k I$, any sub-diagonal element of $T_k = 0$, then $T_k$ can be written in this form and the QR iteration can be performed on $(T_k)_{11}$ and $(T_k)_{22}$ separately (simpler  problems)
\end{itemize}
\textbf{Theorem:} $\lambda(H) = \lambda(H_{11})\cup \lambda(H_{22})$ for $H$ block upper triangular. \textbf{Proof:}
\begin{align*}
    \Longrightarrow& Hx = \lambda x \longrightarrow \begin{bmatrix} H_{11} & H_{12} \\ 0 & H_{22} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
     = \begin{bmatrix} H_{11}x_1 + H_{12}x_2 \\ H_{22}x_2 \end{bmatrix} = \begin{bmatrix} \lambda x_1 \\ \lambda x_2 \end{bmatrix}\\
     &\textrm{and either $x_2=0$ and $\lambda \in \lambda(H_{11})$ or not and $\lambda \in \lambda(H_{22})$}\\
     \Longleftarrow& H_{11}p_1 = \lambda p_1 \longrightarrow \begin{bmatrix} H_{11} & H_{12} \\ 0 & H_{22} \end{bmatrix} \begin{bmatrix} p_1 \\ 0 \end{bmatrix}
     = \begin{bmatrix} H_{11}p_1 \\ 0 \end{bmatrix} = \begin{bmatrix} \lambda p_1 \\ 0 \end{bmatrix}\\
     \Longleftarrow& H_{22}p_2 = \lambda p_2 \longrightarrow \begin{bmatrix} H_{11} & H_{12} \\ 0 & H_{22} \end{bmatrix} \begin{bmatrix} x \\ p_2 \end{bmatrix}
     = \begin{bmatrix} H_{11}x + H_{12}p_2 \\ H_{22}p_2 \end{bmatrix} = \begin{bmatrix} \lambda x_1 \\ 0 \end{bmatrix}\\
     &\textrm{where } H_{11}x + H_{12}p_2 = \lambda x \textrm{ for } x = -(H_{11} - \lambda I)^{-1}H_{12}p_2 \textrm{, making } \lambda \in \lambda(H)
\end{align*}
\textbf{Theorem:} If $H$ is singular unreduced upper Hessenberg, then in QR factorization, $H=QR$, the last row of $R$ is zero.\\
\textbf{Explanation:} When constructing QR iteration, each column of $R$ can be linearly independent from the previous ones (since we're adding a dimension) except for the last one (since $H$ and $R$ must be singluar):
\begin{align*}
    h_1 &= h_{11}e_1 + h_{21}e_2 & h_2 = h_{12}e_1 + h_{22}e_2 + h_{32}e_3 && h_{n-1} = \sum_{i=1}^n h_{n-1,i}e_i
\end{align*}

\subsection{QR iteration on symmetric matrices}
Upper Hessenberg symmetric matrices are tri-diagonal matrices
\begin{itemize}
    \item Unsymmetric case complexity: Transform to upper Hessenberg: $O(n^3)$; QR iteration step: $O(n^2)$; overall QR iteration: $O(pn^3)$, where $p$ is the number of iterations per eval (assume quadratic convergence)
    \item Symmetric case complexity: Transform to upper Hessenberg: $O(n^3)$; QR iteration step: $O(n)$; overall QR iteration: $O(pn^2)$, where $p$ is the number of iterations per eval (assume cubic convergence) 
\end{itemize}


\section{Finding eigenvalues of sparse matrices}
Define \textbf{a sparse matrix} as a matrix with the number of nonzero entries on the order of $O(1)$ (i.e., does not scale with matrix size). The main operation for reducing complexity in this area is matrix-vector multiplication, $Ax = \sum_j a_{ij}x_j$, where you can skip all $a_{ij}$ when $a_{ij} = 0$. 

\subsection{Arnoldi process}
The \textbf{Arnoldi process} is used to reveal the first $k$ eigenvalues of a sparse matrix using a process similar to \textit{Gram-Schmidt}\\
With Arnoldi process, we start with equation $Q^HAQ = H \Longrightarrow AQ = QH$ and use $Q$ to make $H$ where each subsequent column of $AQ$ is made orthogonal to all preceeding columns. The process follows:
\begin{align*}
    1. \; & \textrm{Begin with random } q_1 \in Q \textrm{, such that } \norm{q_1}{2} = 1\\
    \textrm{Iterate through each of the first } & k \textrm{ columns of $Q$ with }\\
    2. \; & Aq_j = \sum_{k = 1}^{j+1} h_{kj}q_k \textrm{, observing we can recover all $h_{ij}$ for $i \leq j$ since } q_i^TAq_j = h_{ij}\\
    3. \; & Aq_j = \sum_{k = 1}^{j} h_{kj}q_k + h_{j+1, j}q_{j+1}\\
    4. \; & r = Aq_j - \sum_{k = 1}^{j} h_{kj}q_k = h_{j+1, j}q_{j+1} \textrm{, where only $r$ is unknown}\\
    5. \;& \norm{q_{j+1}}{2} = 1 \Longrightarrow h_{j+1, j} = \norm{r}{2} \textrm{ and } q_{j+1} = \frac{r}{h_{j+1, j}}
\end{align*}
The output of this process is $k$ columns of $Q$ and the upper $k \times k$ block of upper Hessenberg matrix, $H$, which can be used in the QR iteration to reveal $k$ eigenvalues close to $\lambda(A)$:
\begin{align*}
    AQ &= QH \Longrightarrow AQ_k = Q_kH_k + h_{k+1,k}q_{k+1}e_k^T \textrm{, where } Q_k = Q[:, 1:k], H_k = [1:k, 1:k]\\
    AQ_k &= Q_kX_k\Lambda_k X_k^{-1} + h_{k+1,k}q_{k+1}e_k^T \textrm{, where } H_k = X_k\Lambda_k X_k^{-1} \textrm{ through QR iteration}\\
    A(Q_kX_k) &= (Q_kX_k)\Lambda_k + h_{k+1,k}q_{k+1}x_k^T \textrm{, where } x_k^T \textrm{ is the $k^{th}$ column of } X
\end{align*}
And we get an equation where i) $AQ_k \approx Q_kH_k$, ii) $\Lambda_k$ contains $k$ eigenvalues close to $\lambda_i \in \lambda(A)$, iii) $(Q_kX_k)$ serve as eigenvectors for those eigenvalues, and iv) $h_{k+1,k}q_{k+1}x_k^T$ represents something like an error term.

% KRYLOV SUBSPACES
\subsection{Krylov spaces}
A \textbf{Krylov subspace} is defined as a space of sparse Matrix-vector products: $K(A, q, k) = span\{q_1, Aq_1, A^2q_1, \dots, A^kq_1 \}$\\ \\
In general, Krylov spaces can be used approximate linear algebra problems of $A \in \mathbb{R}^{n \times n}$ in a $K(A, q_1, k) \in \mathbb{R}^{k \times k}$ space instead.

\subsubsection{QR factorization of Krylov subspace contains $Q_k$ from Arnoldi}
\textbf{Proof:} We show for $K_k = Q_kR_k$, that $R_k$ is upper triangular.
\begin{align*}
    \textrm{Start with } Q^TK_k &= R \textrm{ upper triangular for } K_k = \begin{bmatrix}
        \vline & \vline & & \vline \\ q_1 & Aq_1& \dots & A^kq_1 \\ \vline & \vline & & \vline \end{bmatrix}\\
        Q^Tk_j &= Q^TA^{j-1}q_1 = Q^T Q H^{j-1} Q^T q_1 \textrm{, since } A^k = Q^T H^k Q\\
        &= H^{j-1} Q^T q_1 = H^{j-1} e_1 \textrm{, since $Q$ orthogonal}\\
        \Rightarrow r_j \in R &= h_1 \in H^{j-1} \textrm{, which has top $j$ rows nonzero}
\end{align*}
The last statement can be checked by iteratively checking the first column of $H^i$. This result indicates that $Q_kK_k$, produces an upper right triangular matrix since $Q_k$ is the first $k$ columns of $Q$. This also means $Q_k$ forms a basis for $K(A, q_1, k)$.


\subsubsection{Arnoldi process generates a minimal polynomial}
\textbf{Polynomial properties}
\begin{itemize}
    \item If A is diagonalizable, i.e., $A = X\Lambda X^{-1}$, then polynomial $f(A) = Xf(\Lambda)X^{-1}$
    \item \textbf{Characteristic polynomial} of A is $p_A(z) = det(zI - A) = \prod(z - \lambda_i)$ and $p_A(\lambda_i) = 0$ for $\lambda_i \in \lambda(A)$
    \item $f(A) = 0 \Longrightarrow \lambda_i \in \lambda(A)$ are the roots of the polynomial (e.g., $p_A(A) = Xp_A(\Lambda)X^{-1} = 0$
\end{itemize}
Our hope with the Arnoldi process is that for $p_k(H_k) = 0$, revealed in Arnoldi, $p_k(A)$ is minimally small among degree $k-1$ polynomials. Instead of showing $\norm{p_K(A)}{2}$ is minimized (which is hard), we show $\norm{p_K(A)q_1}{2}$ is minimized:
\begin{align*}
    f(x) &= x^k + f_{k-1}x^{k-1} + \dots + f_0 \textrm{, for $f$ that minimizes } \norm{f(A)q_1}{2}\\
    f(A) &= (A^k + f_{k-1}A^{k-1} + \dots + f_0)q_1 = A^kq_1 + K_kf \textrm{, where $f$ is a vector of coefficients}\\
    &= A^kq_1 + Q_ky \textrm{, for some $y$, since $Q_k$ forms a basis for Krylov space}\\
    \textrm{Minimal }\norm{f(A)q_1}{2} &\Longrightarrow \textrm{minimal} \norm{A^kq_1 + Q_ky}{2} \textrm{, so we need to choose $y$ to minimize polynomial}\\
    \textrm{minimal} \norm{A^kq_1 + Q_ky}{2} & \Longrightarrow Q_k^Tf(A)q_1 = 0\\
    Q_k^Tf(A)q_1 &= Q_k^TQf(A)Q^Tq_1 = \begin{bmatrix} I_k & 0 \end{bmatrix} f(H)e_1 = I_kf(H_k)e_1
\end{align*}
This proof shows that $\norm{f(A)q_1}{2}$ is minimal $\Leftrightarrow I_kf(H_k)e_1=0$, the first column of $f(H_k)$ is zero. Now observe that $p_k(H_k)$ achieves this since $p_k(H_k) = 0$. Finally, assuming $K_k$ is full rank, we know $p_k$ must uniquely minimize this norm. 

% LANCZOS PROCESS FOR SYMMETRIC MATRICES
\subsection{Lanczos process}
The \textbf{Lanczos process} is a parallel process to the Arnoldi process, but for symmetric matrices. Reminder: A symmetric upper Hessenberg matrix, $T$ is tri-diagonal. Note this tri-diagonal matrix is of the form
\begin{equation*}
    T = \begin{bmatrix}
        \alpha_1 & \beta_1 & 0 & \dots\\
        \beta_1 & \alpha_2 & \beta_2 & \dots \\
        0 & \ddots & \ddots & \ddots
    \end{bmatrix}
\end{equation*}
The process follows
\begin{align*}
    1. \;& \alpha_k = q_k^TAq_k \Longrightarrow \alpha_kq_k = Aq_k\\
    2. \;& r_k = Aq_k - \beta_{k-1}q_{k-1} - \alpha_kq_k \Longrightarrow r_k = \beta_{k-1}q_{k-1} \textrm{, $r_k$ becomes the orthogonal part of $Aq_k$}\\
    3. \;& \beta_k = \norm{r_k}{2}\\
    4. \;& q_{k+1} = \frac{r_k}{\beta_k}
\end{align*}
The orthogonalization in step 2 is reduced from $O(k)$ in Arnoldi to $O(1)$ in Lanczos because of the symmetry of $A$. Professor Darve notes the "magic" in this. 

\subsubsection{Process for revealing the max eigenvalue of $A$}
\begin{align*}
    \lambda(T_k) &\approx \lambda(A)\\
    \lambda_1 \in \lambda(T_k) &= \max_{x \neq 0} \frac{y^TQ_k^TAQy}{\norm{y}{2}^2} \textrm{, by property that } \lambda_1 \in \lambda(A) = \max_{x \neq 0} \frac{x^TAx}{\norm{x}{2}^2}\\
    &\Longrightarrow \textrm{ want max $x$ of the form } Q_ky \\
    &\Longrightarrow \textrm{ want max $x$ in Krylov space, a subspace of } \mathbb{R}^k\\
    &\Longrightarrow \lambda_1 \in \lambda(T_k) \leq \lambda_1 \in \lambda(A) \textrm{, since it is the max in a smaller space} \\ \\
    \lambda_1 \in \lambda(T_k) &= \max_{x \neq 0} \frac{q_1^Tp(A)Ap(A)q_1}{q_1^Tp(A)^2q_1} \textrm{, and see textbook for step from here to next step}\\
    &\Longrightarrow \lambda_1 \in \lambda(T_k) \leq \lambda_1 - (\lambda_1-\lambda_n)(\frac{\tan(\theta)}{T_{k-1}^{Cheb}(1+2p_1)}) \textrm{, where } p_1 = \frac{\lambda_1 - \lambda_2}{\lambda_2 - \lambda_n}
\end{align*}\
Observe that the $RHS$ approaches $\lambda_1$ when $\lambda_1$ is well separated from the other eigenvalues. 

\section{Iterative splitting methods for solving linear systems}
Use iterative methods to solve linear systems when i) $A$ large and direct methods are computationally expensive, or ii) $A$ is sparse and $Ax$ easily computed. Ideally these methods can reduce cost below $O(n^3)$\\
\textbf{General idea:}
\begin{align*}
    &\textrm{For } A = M - N \textrm{, where } M \textrm{ nonsingular: } Ax = b \Longleftrightarrow Mx - Nx = b \Longleftrightarrow x = M^{-1}Nx + M^{-1}b\\
    &\textrm{And $x$ becomes a fixed point of } f(x) = M^{-1}Nx + M^{-1}b\\
\end{align*}
Fixed points can be identified by iteratively computing $f(x), f(f(x)), \dots$ on starting input, $x$.\\\\
\textbf{Convergence} depends on $M^{-1}N$: Define the error at step $k+1$ as $e_{k+1} = x_{k+1} - x$, then
\begin{align*}
    e_{k+1} &= x_{k+1} - x = M^{-1}Nx_{k} + M^{-1}b - M^{-1}Nx - M^{-1}b\\
    &= M^{-1}Nx_{k} - M^{-1}Nx = M^{-1}N(x_{k} - x) = M^{-1}Ne_k\\
    e_{k+1} &= (M^{-1}N)^ke_0
\end{align*}
And convergence only occurs when the spectral radius of $M^{-1}N$, $\rho(M^{-1}N)<1$. Where $\rho(A) = \max_{\lambda_i \in \lambda(A)} \abs{\lambda_i}$\\
\textbf{Proof:}
\begin{align*}
    \textrm{Let } Gv&=\lambda v \textrm{, where } G = M^{-1}N \textrm{ and pick } x_0 = x + v \textrm{ as a first guess solve}\\
    \textrm{Then } e_k &= G^ke_0 = G^k(x_0 - x) = G^kv = \lambda^kv\\
\end{align*}
And $e_k \rightarrow 0$ if $\abs{\lambda} < 1$.\\\\
The following sections review how to pick $M$ and $N$

\subsection{Jacobi}
\textbf{Definition: }Let $A = D - L - U$, and choose $M = D$ and $N = L+U$. Then we have $Dx_{k+1} = (L+U)x_k + b$\\
\textbf{Convergence: }Converges for any $x_0$ when $A$ is strictly diagonally dominant: $\abs{a_ii} > \sum_{j\neq i} \abs{a_ij}$\\
\textbf{Proof: }
\begin{align*}
    g_{ii} = 0 \textrm{ for } G = M^{-1}N &\Longrightarrow \lambda \in D_i = \{ z \mid \abs{z} \leq \sum_{i\neq j}\abs{g_{ij}}\} \textrm{, by Gershgorin Disc Theorem}\\
    \textrm{Suffice to show } \sum_{i\neq j}\abs{g_{ij}} < 1: \; \;& \sum_{i\neq j}\abs{g_{ij}} < 1 \Longrightarrow \sum_{i\neq j} \frac{\abs{a_{ij}}}{\abs{a_{ii}}} <1 \Longrightarrow \sum_{i\neq j}\abs{a_{ij}} < \abs{a_{ii}}\\
    &\Longrightarrow \textrm{$A$ is strictly diagonoally dominant}
\end{align*}

\subsection{Gauss-Seidel}
\textbf{Definition: } Let $A = D - L - U$, and choose $M = D - L$ and $N = U$. Then we have $(D - L)x_{k+1} = Ux_k + b$\\
\textbf{Householder-John Theorem: } if $A, B \in \mathbb{R}^{m \times n}$ and $A$ and $[A - B - B^T]$ are SPD, then $\rho(H) < 1$ where $H = (A - B)^{-1}B$\\
\textbf{Proof:}
\begin{align*}
    Hx = \lambda x &\Longrightarrow (A - B)^{-1}Bx = \lambda x \Longrightarrow Bx = \lambda (A - B)x \Longrightarrow Bx = \frac{\lambda}{1 + \lambda}Ax\\
    x^HBx &= \frac{\lambda}{1 + \lambda}x^HAx \Longrightarrow x^HB^Tx = \frac{\tilde{\lambda}}{1 + \tilde{\lambda}}x^HA^Tx \textrm{, by taking the complex conjugate of both sides}\\
    0 &< x^HAx - x^HBx - x^HB^Tx = \frac{1 - \abs{\lambda}^2}{\abs{1 + \lambda}^2}x^HAx \textrm{, since $A-B-B^T$ SPD}\\
    0 &< x^HAx \Longrightarrow \abs{\lambda}<1 \textrm{, since $A$ SPD}
\end{align*}
\textbf{Convergence: } If $A$ SPD, then Gauss-Seidel converges for any $x_0$\\
\textbf{Proof:}
Choose $A$ SPD with $A = D - L - U$ and $B = -U = -L^T$
\begin{align*}
    A - B - B^T &= (D - L - U) - (-U) - (-L) = D \textrm{, observe $D$ SPD}\\
    H &= [(D - L - U) - (-U)]^{-1}(-U) = -(D - L)^{-1}U = -G \textrm{, so } \rho(B) = \rho(-G) = \rho(H) < 1
\end{align*}


\subsection{Successive Over-Relaxation (SOR)}
\textbf{Definition:} Using the Gauss-Seidel method, we attempt to increase acceleration using $0<\omega<2$. The  iterative method is $x_{k+1} = x_k + \omega\left[D^{-1}(b + Lx_{k+1} + Ux_k) - x_k\right]$
\begin{align*}
    \textrm{In Gauss-Seidel: }& (D - L)x_{k+1} = b + Ux_k \Longrightarrow x_{k+1} = x_k + D^{-1}(b + Lx_{k+1} + Ux_k) - x_k\\
    & x_{k+1} = x_k + \Delta x_{k+1} \textrm{, where } \Delta x_{k+1} = D^{-1}(b + Lx_{k+1} + Ux_k) - x_k\\
    \textrm{In SOC: }& x_{k+1} = x_k + \omega\Delta x_{k+1} = x_k + \omega(D^{-1}(b + Lx_{k+1} + Ux_k) - x_k)
\end{align*}
SOR is itself a splitting method with $A = D - L - U$,  $M = \frac{1}{\omega}D - L$ and $N = (\frac{1}{\omega} - 1)D + U$\\
\textbf{Convergence:} If $A$ SPD, then SOR converges for any $\omega \in (0, 2)$\\
\textbf{Proof:}
In the Householder-John Theroem, choose $A_{HJ} = \omega A$ and $B_{HJ} = (\omega - 1)D - \omega U$
\begin{align*}
    A \textrm{ SPD and } A_{HJ} - B_{HJ} - B_{HJ}^T = (2-\omega)D \textrm{ SPD if } 0 < \omega < 2\\
    \textrm{Then }(A_{HJ} - B_{HJ})^{-1}B_{HJ} = (D -\omega L)^{-1}((\omega - 1)D - \omega U) = G_{SOR}\\
    \textrm{Then } \rho(G_{SOR}) = \rho((A_{HJ} - B_{HJ})^{-1}B_{HJ}) = \rho(H)<1
\end{align*}
Also, for $\omega \notin (0, 2)$ there exists $x_0$ s.t. SOR will not converge\\
\textbf{Proof:}
\begin{align*}
    det(G) &= det(M^{-1}N) = \frac{det((1-\omega)D + \omega U)}{det(D - \omega L)} = \frac{\prod_i(1-\omega)d_{ii}}{\prod_i d_{ii}} = (1-\omega)^n \textrm{, since $L, U$ triangular with 0 on diagonal}\\
    det(G) &= \prod_{\lambda_i \in \lambda(G)}\lambda_i = (1-\omega)^n \Longrightarrow \abs{\lambda_{max}(G)}\geq \abs{1 - \omega} \Longrightarrow \textrm{Convergence only when } \abs{1 - \omega} < 1
\end{align*}

\subsection{Chebyshev Semi-iterative Method}
The most efficient splitting method, but requires us knowing the interval containing $\lambda(A)$. It also uses knob, $\omega$, but we can choose to update this at each step\\
\textbf{Definition: }With $A = M - N$, $G = M^{-1}N$, and $\omega_k \in \mathbb{R}$, the iterative method is
\begin{align*}
    x_{k+1} &= x_k + \omega_k((M^{-1}b + Gx_k) - x_k) = x_k + \omega_kM^{-1}(b - Ax_k)\\
    \textrm{With } e_k &= (I - \omega_{k-1}M^{-1}A)e_{k-1} = \left(\prod_{i=0}^{k-1}(I - \omega_{i}M^{-1}A)\right)e_0
\end{align*}
We can minimize error $e_k$ with $\norm{q_k(M^{-1}A)e_0}{2}$, where $q_k(x) = (1 - \omega_{k-1}x)\dots(1 - \omega_0x)$. 
\begin{align*}
    \norm{e_0}{2} =& \norm{q_k(M^{-1}A)e_0}{2} \leq \max_\lambda q_k(\lambda)\norm{e_0}{2}\\
    &\textrm{Recalling } q_k(M^{-1}A) = Xq_k(\Lambda)X^{-1} \textrm{ for } M^{-1}A = X\Lambda X^{-1} \textrm{ with each diagonal element in } q_k(\Lambda) = q_k(\lambda_i)\\
    \norm{e_k}{2} \leq& \max_\lambda q_k(\lambda)\norm{e_0}{2} \leq \frac{\norm{e_0}{2}}{T_k\left(\frac{\beta + \alpha}{\beta - \alpha}\right)} \textrm{, where } \abs{q_k(x)} \leq \frac{1}{T_k\left(\frac{\beta + \alpha}{\beta - \alpha}\right)}
\end{align*}

\section{Iterative Krylov methods for solving linear systems}
Motivation: We can look for solutions to $Ax=b$ in the increasing dimensions of the Krylov subspace.\\
At a given step in splitting methods we have
\begin{equation*}
    x_k = b + (I - A)x_{k-1} = x_{k-1} + b - Ax_{k-1} \textrm{, for } M = I
\end{equation*}
Unrolling the iterations, we see $x_k$ is a linear combination of $\{b, Ab, \dots, A^{k-1}b\}$, a Krylov Subspace of $A$. We can write 
\begin{itemize}
    \item $x_k = Q_ky$ for $Q_k$ the orthonormal basis of $\mathcal{K}_k(A, b, k)$
    \item $r_k = b - Ax_k = b - AQ_ky$
    \item Minimizing $r_k \Longleftrightarrow r_k \perp \mathcal{K}(A, b, k)$. 
\end{itemize}


\subsection{Conjugate Gradient}
Krylov method for SPD matrices. Error, $x - Q_ky$ minimized in $A$ norm. Residual, $r_k$, minimized in $A^{-1}$ norm. O(n) per iteration.

\subsubsection{Naive approach to CG}
\begin{align*}
    \textrm{Minimize } \norm{r_k}{{A^{-1}}}^2 = (b - AQ_ky)^TA^{-1}(b - AQ_ky) = b^Tx - 2y^TQ_k^Tb + y^TQ_k^TAQ_ky\\
    \frac{d}{dy}\left(b^Tx - 2y^TQ_k^Tb + y^TQ_k^TAQ_ky\right) = 0 \Longrightarrow Q^T_kAQ_ky = Q^T_kb \textrm{, minimizes y}
\end{align*}
And the iterative process becomes
\begin{itemize}
    \item Construct $Q_k$ from Lanczos process (O(n))
    \item Compute $y$ from $Q^T_kAQ_ky = Q^T_kb = \norm{b}{2}e_1$ (O(k), since $H_k$ assembled in Lanczos)
    \item Compute $x_k = Q_ky$ (O(kn), the expensive step we'll try to simplify)
\end{itemize}

\subsubsection{Efficient approach to CG}
\textbf{Search directions:} We increase efficiency by working with search directions, $\Delta x_k = x_{k+1} - x_k$, instead of $x_{k+1}$ directly\\
Search directions, $\Delta x_k, \Delta x_l$ are $A$-conjugate: $(\Delta x_k)^TA\Delta x_l = 0$ for $k \neq l$\\
\textbf{Proof:}
\begin{align*}
    r_k - r_{k+1} = (b - Ax_k) - (b - Ax_{k+1}) = Ax_{k+1} - Ax_k &= A \Delta x_k \\
    \textrm{Since } r_k, r_{k+1} \perp Q_k &\Longrightarrow A \Delta x_k \perp Q_k\\
    \Delta x_l = x_{l+1} - x_l \in \mathcal{K}_{l+1} \textrm{ and } \Delta x_k = x_{k+1} - x_k \in \mathcal{K}_{k+1} &\Longrightarrow \textrm{For } l < k, A\Delta x_k\perp \Delta x_l \Rightarrow \Delta x_l^TA\Delta x_k = 0\\
    \Delta x_l^TA\Delta x_k = (\Delta x_l^TA^T)\Delta x_k &\Longrightarrow A\Delta x_l\perp \Delta x_k \textrm{, since $A$ SPD}\\
    &\therefore (\Delta x_k)^TA\Delta x_l = 0 \textrm{ for } k \neq l
\end{align*}
We can work with search directions directly using the following equalities
\begin{equation*}
    \Delta x_k = \mu_{k+1}p_{k+1} \;\;\;\;\; p_{k+1} = r_k + \sum_{l=1}^k\tau_{lk}p_l
\end{equation*}
Determined by
\begin{align*}
    p_{k+1}' &\in span\{r_0, \dots, r_k\} = span\{p_1, \dots, p_k, r_k\} \Longrightarrow p_{k+1}' = \alpha_kr_k + \sum_{l=1}^k \tau_{lk}'p_l' \textrm{ for some } \alpha_k, \tau_{lk}'\\
    p_{k+1} &= r_k + \sum_{l=1}^k \tau_{lk}p_l \textrm{, setting } \alpha_k = \mu_{k+1}, \;\;  p_{k+1} = \frac{1}{\mu_{k+1}}p_{k+1}' \textrm{, and } \tau_{lk} = \frac{\mu_l}{\mu_k}\tau_{lk}'
\end{align*}
Let $p_k' = \Delta x_{k-1}$. We rely on several properties to achieve the above\\
\begin{itemize}
    \item \textbf{Property:} when $x_{k+1} = x_k \Rightarrow r_{k+1} = r_k \Rightarrow r_k = 0$ since $r_k \in \mathcal{K}_{k+1} \perp r_{k+1}$
    \item \textbf{Property:} $span\{p_1', \dots, p_l'\} = \mathcal{K}_l$
    \begin{itemize}
        \item $x_k, x_{k-1} \in \mathcal{K}_k \rightarrow p_k' \in \mathcal{K}_k$
        \item $x_k \neq x_{k-1} \rightarrow \Delta x_{k-1} \neq 0 \rightarrow p_k' \notin \mathcal{K}_{k-1}$
        \item $p_k' \in \mathcal{K}_k$ and $p_k' \notin \mathcal{K}_l$ for $l < k \Longrightarrow span\{p_1', \dots, p_l'\} = \mathcal{K}_l$
    \end{itemize}
    \item \textbf{Property:} $span\{r_0, \dots, r_{l-1}\} = \mathcal{K}_l$
    \begin{itemize}
        \item $b, Ax_{k-1} \in \mathcal{K}_k \Longrightarrow r_{k-1} = b-Ax_{k-1} \in \mathcal{K}_k$
        \item $r_{k+1} \neq r_k \Longrightarrow r_{k-1} \notin \mathcal{K}_{k-1}$
        \item $r_{k-1} \in \mathcal{K}_k$ and $r_{k-1} \notin \mathcal{K}_{l}$ for $l < k \Longrightarrow span\{r_0, \dots, r_{l-1}\} = \mathcal{K}_l$
    \end{itemize}
    \item \textbf{Property:} $span\{r_0, \dots, r_{l-1}\} = \mathcal{K}_l = span\{p_1', \dots, p_l'\}$
    \item \textbf{Property:} $\mu_{k+1} = \frac{r_k^Tr_k}{p_{k+1}^TAp_{k+1}}$
    \begin{align*}
        &r_k - r_{k+1} = A\Delta x_{k+1} = \mu_{k+1}Ap_{k+1}\\
        &p_{k+1}^Tr_k = \mu_{k+1}p_{k+1}^TAp_{k+1} \textrm{, since } - p_{k+1}^Tr_{k+1} =0\\
        &\mu_{k+1} = \frac{p_{k+1}^Tr_k}{p_{k+1}^TAp_{k+1}} = \frac{r_k^Tr_k}{p_{k+1}^TAp_{k+1}} \textrm{, since } p_{k+1} = r_k + \tau_kp_k \Longleftrightarrow r_k^Tp_{k+1} = r_k^Tr_k
    \end{align*}
    \item \textbf{Property:} $\tau_k = \frac{r_k^Tr_k}{r_{k-1}^Tr_{k-1}}$
    \begin{align*}
        &p_{k+1} = r_k + \sum_{l=1}^k \tau_{lk}p_l\\
        &p_k^TAp_{k+1} = p_k^TAr_k + \sum_{l=1}^k \tau_{lk}p_k^TAp_l \Longleftrightarrow 0 = p_k^TAr_k + \tau_{kk}p_k^TAp_k\\
        &\tau_k = \frac{-p_k^TAr_k}{p_k^TAp_k} = \frac{r_k^Tr_k}{r_{k-1}^Tr_{k-1}} \textrm{, since } Ap_k = \frac{1}{\mu_k}(r_{k+1} - r_k) \Longleftrightarrow r_k^TAp_k = \frac{r_k^Tr_k}{\mu_k} \textrm{ with } \mu_k = \frac{r_{k-1}^Tr_{k-1}}{p_k^TAp_k}
    \end{align*}
\end{itemize}

\subsubsection{Key orthogonality results}
\begin{itemize}
    \item $r_k \perp Q_k$, by construction of $r_k$ to minimize $\norm{r_k}{{A^{-1}}}$
    \item $r_k \perp r_l$ for $l \neq k$, since $span\{r_0, \dots, r_{l-1}\} = \mathcal{K}_l$
    \item $p_k \perp Ap_l$ for $l \neq k$, since $r_k, r_{k-1} \perp \mathcal{K}_{k-1}$ and $Ap_k = \frac{1}{\mu_k}A\Delta x_k = \frac{1}{\mu_k}A(r_k - r_{k-1})$ then $Ap_k \perp \mathcal{K}_{k-1}$, but for $l < k$, $p_l = \frac{1}{\mu_l}(x_l - x_{l-1}) \in \mathcal{K}_{k-1}$ so $p_l \perp Ap_k$. We can get same result for $l>k$ using $A$ SPD properties.
    \item $r_k \perp p_l$ for $l \leq k$, TODO
    \item $r_k \perp Ap_l$ for $l < k$ since $p_l^TAr_k = (Ap_l)^Tr_k$ with $Ap_l = \frac{1}{\mu_l}(x_l - x_{l-1}) \in \mathcal{K}_{l+1}$ and $r_k \perp \mathcal{K}_{l+1}$ for $l+1 < k+1 \Leftrightarrow l < k$
\end{itemize}

\subsubsection{Congugate Gradient algorithm}
\begin{align*}
    (i) & \;\textrm{Choose some } x_0 \textrm{ (could be $x_0 = 0$)}\\
    (ii) & \; r_0 = b-Ax_0, \; p_0 = 0, \; k = 1\\
    (iii) & \; \textrm{While } r_{k-1} \neq 0\\
    &\tau_{k-1} = \frac{\norm{r_{k-1}}{2}^2}{\norm{r_{k-1}}{2}^2}\;,\;\;\; p_k = r_{k-1} + \tau_{k-1}p_{k-1}\;, \;\;\; \mu_k = \frac{\norm{r_{k-1}}{2}^2}{p_k^TAp_k}\;, \;\;\; x_k = x_{k-1} + \mu_kp_k\;, \;\;\;r_k = r_{k-1} - \mu_kAp_k\\
    &k \leftarrow k+1\\
    (iv) & \; \textrm{Return } x_{k-1}
\end{align*}

\subsection{GMRES}
Krylov method for general matrices, Generalized Minimal Residual Method (GMRES). Error, $x-x_k = x - Q_ky$ minimized in $A^TA$ norm. Residual, $r_k$, minimized in 2-norm. O(kn) per iteration.
\begin{align*}
    \norm{x - x_k}{{A^TA}}^2 = (x - x_k)^TA^TA(x - x_k) = (b - Ax_k)^T(b - Ax_k) = \norm{b - Ax_k}{2}^2 = \norm{r_k}{2}^2
\end{align*}
We can use least squares methods to minimize $y$ in $\norm{r_k}{2}^2$:
\begin{align*}
    \norm{r_k}{2}^2 &= \norm{b - Ax_k}{2}^2 = \norm{Q_{k+1}Q_{k+1}^Tb - Q_{k+1}Q_{k+1}^TAQ_ky}{2}\\
    &=\norm{Q_{k+1}(Q_{k+1}^Tb - Q_{k+1}^TAQ_ky)}{2} =\norm{Q_{k+1}^Tb - Q_{k+1}^TAQ_ky}{2} =\norm{\norm{b}{2}e_1 - H_ky}{2}
\end{align*}
Lastly, use givens rotations to make $H_k$ upper triangular, and then find solution $y_k$.

\subsection{Preconditioning}
The condition number and distribution of eigenvalues of $A$ influence convergence in both CG and GMRES, with clustered eigenvalues leading to faster convergence. Matrices can be conditioned, with the tradeoff being i) cost of applying a matrix preconditioner vs ii) savings from reduced iterations
\begin{itemize}
    \item Left preconditioning: $(M_1A)x = M_1b$
    \item Right preconditioning: $(AM_2)z = b$, and solve $x = M_2z$
    \item Symmetric preconditioning: $(M_1AM_2)z = M_1b$, and solve $x = M_2z$
\end{itemize}

\subsubsection{CG predonditioning}
Choose $M$ SPD and define $C^2 = M$. CG preconditioning follows symmetric preconditioning, solving $CACy = Cb$ and $Cy=x$, but with tricks that only require computing $MA$.
\begin{itemize}
    \item $MA$ is similar to $CAC$ (suffice to show $\lambda(MA) = \lambda(CAC)$)
    \begin{align*}
        MAx = \lambda x &\Leftrightarrow CCACz = \lambda Cx \Leftrightarrow CACz = \lambda z \textrm{, for } x = Cz\\
        CACy = \lambda y &\Leftrightarrow MACy = \lambda Cy \Leftrightarrow MAx = \lambda x \textrm{, for x = Cy}
    \end{align*}
    \item $MAx = Mb \Longleftrightarrow (CAC)C^{-1} = Cb$
    \begin{align*}
        MAx = Mb \Leftrightarrow CCAx = CCb \Leftrightarrow CACC^{-1}x = Cb
    \end{align*}
    \item $C^{-1}x \in \mathcal{K}(CAC, Cb, k) \Longleftrightarrow x \in \mathcal{K}(MA, Mb, k)$
    \begin{align*}
        C^{-1}x \in \mathcal{K}(CAC, Cb, k) &\Longleftrightarrow C^{-1}x = \alpha_0Cb + \sum_{i=1}^k\alpha_i(CAC)^iCb = \alpha_0Cb + \sum_{i=1}^k\alpha_iCA(MA)^{i-1}CCb\\
        &\Longleftrightarrow x = \alpha_0Mb + \sum_{i=1}^k\alpha_iMA(MA)^{i-1}Mb = \sum_{i=1}^k\alpha_i(MA)^iMb \Longleftrightarrow x \in \mathcal{K}(MA, Mb, k)
    \end{align*}
\end{itemize}

\section{Direct methods}
Direct methods are an alternative to iterative methods for solving linear systems, and take advantage of sparse data formatting.
\subsection{Matrix storage}
\begin{equation*}
    A = \begin{bmatrix}
        4.1 & 0 & 2.9  & 0\\
        1.2&-0.3 & 0 & -0.1\\
        0 & 7.2 & 9.2 & 0\\
        0 & 0 & 0 & 1.0
    \end{bmatrix}
\end{equation*}
\textbf{Coordinate format (COO):} $(i, j, a_{ij})$, e.g. $COO(A) = (1, 1, 4.1), (1, 3, 2.9), \dots, (4, 4, 1.0)$\\
\textbf{Compressed Sparse Row (CSR):} $\{nzval, colval, rowptr\}$, e.g., 
\begin{equation*}
    CSR(A) = \begin{cases}
        nzval &= [4.1, 2.9, \dots, 1.0] \textrm{ (nonzero values)}\\
        colval &= [1, 3, \dots, 4]\textrm{ (column values)}\\
        rowptr &= [1, 3, 6, 8, 9] \textrm{ (index of nzval that starts each row)}
    \end{cases}
\end{equation*}
\textbf{Compressed Sparse Column (CSC):} $\{nzval, rowval, colptr\}$, e.g., 
\begin{equation*}
    CSR(A) = \begin{cases}
        nzval &= [4.1, 1.2, \dots, 1.0] \textrm{ (nonzero values)}\\
        rowval &= [1, 2, \dots, 4]\textrm{ (row values)}\\
        colptr &= [1, 3, 5, 7, 9] \textrm{ (index of nzval that starts each column)}
    \end{cases}
\end{equation*}
\textbf{Example matrix-vector product, $Ax$:}

\begin{lstlisting}
for i=1:A.m
    y[i] = 0.0
    for k=A.rowptr[i]:A.rowptr[i+1]-1
        y[i] += A.nzval[k]*x[A.colval[k]]
    end
end
\end{lstlisting}
\textbf{Conceptualizing the graph of $A, G_A$:} Edge $j \rightarrow i$ exists if $a_{ij} \neq 0$
\subsection{Solving triangular sparse systems}
\textbf{Dense $b$:} For $Lx=b$ with sparse lower triangular matrix, $L$, and dense vector, $b$: $x_i = \frac{1}{l_{ii}}\left(b_i - \sum_{j=1}^{i-1}l_{ij}x_j\right)$
\begin{lstlisting}
for i=1:A.m
    x[i] = b[i]
    for k=A.rowptr[i]:A.rowptr[i+1]-2
        x[i] -= A.nzval[k]*x[A.colval[k]]
    end
    x[i] /= A.nzval[A.rowptr[i+1]-1]
end
\end{lstlisting}
\textbf{Sparse $b$:} We can improve on this code when $b$ sparse by first determining the nonzero pattern of $x$ (nontrivial). Once we have the nonzero pattern of $x$, we can run the above code on sparse $x$. Observing the equation above we see the following rules for the nonzero pattern in $x$
\begin{equation*}
    x_i = \frac{1}{l_{ii}}\left(b_i - \sum_{j=1}^{i-1}l_{ij}x_j\right) \Longrightarrow 
    \begin{cases}
        b_i \neq 0 & \Rightarrow x_i \neq 0\\
        \exists j<i \textrm{ s.t. } l_{ij} \neq 0 \textrm{ and } x_j \neq 0 & \Rightarrow x_i \neq 0
    \end{cases}
\end{equation*}
Using \textbf{graph theory}, the statements above are equivalent to saying
\begin{itemize}
    \item $X$ is the set of nodes reachable from $B$, the set of nodes for which $b_i \neq 0$, on $G_L$
    \item The reach of node $j$ is  all $i$ for which there is a path $j \rightsquigarrow i$
    \item Use recursive backtracking ("depth-first search") to determine set $X$.
\end{itemize}

\subsection{Cholesky factorization}
Direct methods for computing sparse Cholesky factorizations, $A=LL^T$

\subsubsection{Up-looking Cholesky factorization}
Starting with known $L'$, an upper $k \times k$ block of $L$, we can use the \textbf{up-looking Cholesky factorization} to determine the $(k+1)^{st}$ row/column of $L$
\begin{align*}
    (i) \;& \textrm{For $A$ SPD, initialize } L' = \sqrt{a_{11}}\\
    (ii) \;& \textrm{For } k=2, \dots, n, \textrm{write top $k\times k$ block as } \begin{bmatrix} L' & 0\\x^T & w \end{bmatrix} \begin{bmatrix} L'^T & x\\0 & w \end{bmatrix} = \begin{bmatrix} A' & b\\b^T & a \end{bmatrix}\\
    &\textrm{Solve } L'x=b \textrm{ for $x$, compute } w = \sqrt{a-x^Tx} \textrm{, and update/return } L' = \begin{bmatrix} L' & 0\\x^T & w \end{bmatrix}
\end{align*}
\textbf{Notice } $L'_{k-1}l_k^T = a_k^T$, so we can leverage the same nonzero pattern relationship as above:
\begin{equation*}
    l_{ij} = \frac{1}{l_{ii}}\left(a_{ij} - \sum_{k=1}^{j-1}l_{jk}l_{ik}\right) \Longrightarrow \textrm{For } j < i
    \begin{cases}
        a_{ij} \neq 0 & \Rightarrow l_{ij} \neq 0\\
        \exists k<j \textrm{ s.t. } l_{jk} \neq 0 \textrm{ and } l_{ik} \neq 0 & \Rightarrow l_{ij} \neq 0
    \end{cases}
\end{equation*}

\subsubsection{Elimination trees}
The above procedure takes $A$ and produces $G_L$, wich reflects the nonzero pattern of $L$ in $A=LL^T$. As a special property of $A$ SPD, we can construct a more simple \textbf{Elimination Tree}, $E_T$, a graph of the first nonzero off-diagonal element in each column of $L$.\\\\
\textbf{The elimination tree has the same reach as $G_L$:} 
\begin{itemize}
    \item For any $j$, let $i'$ be the smallest row index s.t., $L_{i'j}  \neq 0$. We show removing $j\rightarrow i$ from $G_L$, with $i > i'$ does not change the reach of $G_L$
    \item \textbf{Proof:} Consider $k \in Reach(j)$ and how/if it changes after we remove edge $j \rightarrow i$:
    \begin{itemize}
        \item If $i$ wasn't in the path of $j \rightsquigarrow k$, then the reach is unchanged
        \item If $i$ was in path of $j \rightarrow i \rightsquigarrow k$, the reach is still unchanged because $l_{ij}\neq 0$ and $l_{i'j} \neq 0 \Longrightarrow l_{i'i} \neq 0$, so a path, $j \rightsquigarrow k$, can still be constructed: $j \rightarrow i' \rightarrow i \rightsquigarrow k$
    \end{itemize}
\end{itemize}
\subsubsection{Worked example of $A \longrightarrow E_T \longrightarrow G_L$:}
$A \longrightarrow E_T$
\begin{align*}
    A = \begin{bmatrix}
        X &- &X &X &X\\
        - &X &- &- &X\\
        X &- &X &- &-\\
        X &- & -& X &-\\
        X &X &- &- &X
    \end{bmatrix}
    \longrightarrow E_T = \begin{cases}
        i=1: & a_{11}\neq 0 \Rightarrow \circled{1} \textrm{ (only looking at or below the diagonal } \forall i)\\
        i=2: & \circled{1}\; \mid \; a_{22}\neq 0 \Rightarrow \circled{2}\\
        i=3: & a_{31}\neq 0 \Rightarrow \circled{1} \rightarrow \circled{3} \; \mid \; \circled{2}\\
        i=4: & a_{41} \neq 0 \Rightarrow \circled{1} \rightarrow \circled{3} \rightarrow \circled{4} \; \mid \; \circled{2}\\
        i=5: & a_{51} \neq 0 \Rightarrow \circled{1} \rightarrow \circled{3} \rightarrow \circled{4} \rightarrow \circled{5} \leftarrow \circled{2} \Leftarrow a_{52}\neq 0 \textrm{ (final $E_T$)}
    \end{cases}
\end{align*}
$E_T \longrightarrow G_L$: (Relying on $L'_{k-1}l_k^T = a_k^T$)
\begin{align*}
    &\begin{cases}
        i=1: l_{11}l_{21} = a_{12} \Longleftrightarrow [X][l_{21}] = [-]& a_{12} = 0 \Rightarrow l_{21} = 0 \\
        i=2: \begin{bmatrix} X & -\\ - & X \end{bmatrix} \begin{bmatrix} l_{31} \\ l_{32} \end{bmatrix} = \begin{bmatrix} a_{13}\\a_{23} \end{bmatrix} = \begin{bmatrix} X\\- \end{bmatrix}& a_{13}\neq 0 \Rightarrow l_{31} \neq 0\\
        i=3: \begin{bmatrix} X & - & -\\ - & X & -\\X & - & X \end{bmatrix} \begin{bmatrix} l_{41} \\ l_{42} \\ l_{43} \end{bmatrix} = \begin{bmatrix} a_{14}\\a_{24} \\a_{34}\end{bmatrix} = \begin{bmatrix} X\\-\\- \end{bmatrix} & a_{14}\neq 0 \Rightarrow l_{41} \neq 0 \Longrightarrow l_{31}\neq 0 \Rightarrow l_{43}\neq 0 \textrm{, since $3$ in reach of $1 \in G_L$}\\
        i=4: \begin{bmatrix} X & - & -&-\\ - & X & -&-\\X & - & X &- \\X&-&X&X\end{bmatrix} \begin{bmatrix} l_{51} \\ l_{52} \\ l_{53}\\l_{54} \end{bmatrix} = \begin{bmatrix} a_{15}\\a_{25} \\a_{35}\\a_{45}\end{bmatrix} = \begin{bmatrix} X\\X\\-\\- \end{bmatrix} & 
        \begin{matrix}
            a_{15}\neq 0 \Rightarrow l_{51} \neq 0 \Longrightarrow l_{53}, l_{54} \neq 0 \textrm{ since $3,4$ in reach of $1 \in G_L$}\\
            a_{25}\neq 0 \Rightarrow l_{52} \neq 0 \textrm{ (but nothing else in reach of $2 \in G_L$)}
        \end{matrix}
    \end{cases}\\
    G_L &= \begin{bmatrix} X & - & -&-&-\\ - & X & -&-&-\\X & - & X &-&- \\
        X&-&X&X&-\\X&X&X&X&X\end{bmatrix}
\end{align*}
\textbf{Notice: } $E_T \subseteq G_A$, $E_T \subseteq G_L$, $G_A \subseteq G_L$ (when undirected)

\end{document}
